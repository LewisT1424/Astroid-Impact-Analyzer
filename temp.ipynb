{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd360928",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import json\n",
    "import numpy as np\n",
    "pl.Config.set_fmt_float(\"mixed\")\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, f1_score\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8d60ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/raw/astroid_2010-01-01_2010-01-08.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "with open('data/raw/astroid_2010-01-08_2010-01-15.json', 'r') as af:\n",
    "    data_2 = json.load(af)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5727e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_data(data):\n",
    "    data_list = []\n",
    "    for date in data['near_earth_objects']:\n",
    "        result = {'date': date}\n",
    "        for idx, obj in enumerate(data['near_earth_objects'][date]):\n",
    "            new_result = result | {\n",
    "                # General stuff about astroid\n",
    "                'obj_that_day': idx,\n",
    "                'id': str(obj['id']),\n",
    "                'name': obj['name'],\n",
    "                'absolute_magniutude_h': obj['absolute_magnitude_h'],\n",
    "                # Estimated Diameter\n",
    "                'estimated_diameter_min_km': obj['estimated_diameter']['kilometers']['estimated_diameter_min'],\n",
    "                'estimated_diameter_max_km': obj['estimated_diameter']['kilometers']['estimated_diameter_max'],\n",
    "                'estimated_diameter_min_m': obj['estimated_diameter']['meters']['estimated_diameter_min'],\n",
    "                'estimated_diameter_max_m': obj['estimated_diameter']['meters']['estimated_diameter_max'],\n",
    "                'estimated_diameter_min_miles': obj['estimated_diameter']['miles']['estimated_diameter_min'],\n",
    "                'estimated_diameter_max_miles': obj['estimated_diameter']['miles']['estimated_diameter_max'],\n",
    "                'estimated_diameter_min_feet': obj['estimated_diameter']['feet']['estimated_diameter_min'],\n",
    "                'estimated_diameter_max_feet': obj['estimated_diameter']['feet']['estimated_diameter_max'],\n",
    "                # Potentially hazardous, Sentry object refers to if the astroid is tracked by nasa's sentry system \n",
    "                'is_potentially_hazardous': obj['is_potentially_hazardous_asteroid'],\n",
    "                'is_sentry_object': obj['is_sentry_object']\n",
    "            }\n",
    "            for val in obj['close_approach_data']:\n",
    "                final_result = new_result | {\n",
    "                    # Close approaching dates\n",
    "                    'close_approach_date': val['close_approach_date_full'],\n",
    "                    'epoch_date_close_approach': val['epoch_date_close_approach'],\n",
    "                    # Velocity values\n",
    "                    'relative_velocity_km/sec': float(val['relative_velocity']['kilometers_per_second']),\n",
    "                    'relative_velocity_km/hr': float(val['relative_velocity']['kilometers_per_hour']),\n",
    "                    'relative_velocity_mph': float(val['relative_velocity']['miles_per_hour']),\n",
    "                    # Miss distance\n",
    "                    'miss_distance_astronomical': float(val['miss_distance']['astronomical']),\n",
    "                    'miss_distance_lunar': float(val['miss_distance']['lunar']),\n",
    "                    'miss_distance_kilometers': float(val['miss_distance']['kilometers']),\n",
    "                    'miss_distance_miles': float(val['miss_distance']['miles']),\n",
    "                    # Orbiting body \n",
    "                    'oribiting_body': val['orbiting_body']\n",
    "                }\n",
    "\n",
    "                data_list.append(final_result)\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0f5672e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pl.DataFrame(format_data(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a021f297",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['date',\n",
       " 'obj_that_day',\n",
       " 'id',\n",
       " 'name',\n",
       " 'absolute_magniutude_h',\n",
       " 'estimated_diameter_min_km',\n",
       " 'estimated_diameter_max_km',\n",
       " 'estimated_diameter_min_m',\n",
       " 'estimated_diameter_max_m',\n",
       " 'estimated_diameter_min_miles',\n",
       " 'estimated_diameter_max_miles',\n",
       " 'estimated_diameter_min_feet',\n",
       " 'estimated_diameter_max_feet',\n",
       " 'is_potentially_hazardous',\n",
       " 'is_sentry_object',\n",
       " 'close_approach_date',\n",
       " 'epoch_date_close_approach',\n",
       " 'relative_velocity_km/sec',\n",
       " 'relative_velocity_km/hr',\n",
       " 'relative_velocity_mph',\n",
       " 'miss_distance_astronomical',\n",
       " 'miss_distance_lunar',\n",
       " 'miss_distance_kilometers',\n",
       " 'miss_distance_miles',\n",
       " 'oribiting_body']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33571665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering\n",
    "def feature_engineering(data: pl.DataFrame) -> pl.DataFrame:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97850477",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = feature_engineering(data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de03915f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 70)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>date</th><th>obj_that_day</th><th>id</th><th>name</th><th>absolute_magniutude_h</th><th>estimated_diameter_min_km</th><th>estimated_diameter_max_km</th><th>estimated_diameter_min_m</th><th>estimated_diameter_max_m</th><th>estimated_diameter_min_miles</th><th>estimated_diameter_max_miles</th><th>estimated_diameter_min_feet</th><th>estimated_diameter_max_feet</th><th>is_potentially_hazardous</th><th>is_sentry_object</th><th>close_approach_date</th><th>epoch_date_close_approach</th><th>relative_velocity_km/sec</th><th>relative_velocity_km/hr</th><th>relative_velocity_mph</th><th>miss_distance_astronomical</th><th>miss_distance_lunar</th><th>miss_distance_kilometers</th><th>miss_distance_miles</th><th>oribiting_body</th><th>avg_diameter_km</th><th>diameter_uncertainty_km</th><th>estimated_volume</th><th>cross_section_area_km2</th><th>diameter_uncertainty_ratio</th><th>size_category</th><th>kenetic_energy</th><th>momentum</th><th>velocity_per_au</th><th>velocity_distance_ratio</th><th>velocity_category</th><th>lunar_distance_ratio</th><th>earth_radii_distance</th><th>close_approach_score</th><th>impact_potential</th><th>destruction_potential</th><th>hazard_index</th><th>proximity_level</th><th>approach_datetime</th><th>approach_year</th><th>approach_month</th><th>approach_day</th><th>approach_hour</th><th>day_of_week</th><th>day_of_year</th><th>month_sin</th><th>month_cos</th><th>hour_sin</th><th>hour_cos</th><th>brightness_size_ratio</th><th>apparent_density_inverse</th><th>brightness_category</th><th>size_velocity_product</th><th>size_squared_velocity</th><th>escape_velocity_ratio</th><th>threat_score</th><th>size_percentile</th><th>velocity_percentile</th><th>distance_percentile</th><th>size_zscore</th><th>velocity_zscore</th><th>distance_zscore</th><th>log_diameter</th><th>log_velocity</th><th>log_distance</th></tr><tr><td>str</td><td>i64</td><td>str</td><td>str</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>bool</td><td>bool</td><td>str</td><td>i64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>str</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>str</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>str</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>str</td><td>datetime[ms]</td><td>i32</td><td>i8</td><td>i8</td><td>i8</td><td>i8</td><td>i16</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>str</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;2020-01-01&quot;</td><td>0</td><td>&quot;3564720&quot;</td><td>&quot;(2011 HS60)&quot;</td><td>21.34</td><td>0.143402</td><td>0.320656</td><td>143.401923</td><td>320.656449</td><td>0.089106</td><td>0.199247</td><td>470.478767</td><td>1052.022504</td><td>false</td><td>false</td><td>&quot;2020-Jan-01 21:59&quot;</td><td>1577915940000</td><td>17.774433</td><td>63987.957021</td><td>39759.628072</td><td>0.198878</td><td>77.363533</td><td>2.9752e7</td><td>1.8487e7</td><td>&quot;Earth&quot;</td><td>0.232029</td><td>0.177255</td><td>0.006541</td><td>0.042284</td><td>0.763932</td><td>&quot;medium&quot;</td><td>2.066417</td><td>0.116258</td><td>89.373558</td><td>0.597425</td><td>&quot;medium&quot;</td><td>0.000201</td><td>4669.866872</td><td>5.028209</td><td>20.737274</td><td>10.390376</td><td>5.7170e-7</td><td>&quot;moderate&quot;</td><td>2020-01-01 21:59:00</td><td>2020</td><td>1</td><td>1</td><td>21</td><td>3</td><td>1</td><td>0.5</td><td>0.866025</td><td>-0.707107</td><td>0.707107</td><td>91.971188</td><td>152.888045</td><td>&quot;bright&quot;</td><td>4.124187</td><td>0.956932</td><td>1.587003</td><td>20.633524</td><td>0.865079</td><td>0.793651</td><td>0.547619</td><td>0.493881</td><td>0.669826</td><td>-0.049103</td><td>0.208663</td><td>2.932496</td><td>17.208398</td></tr><tr><td>&quot;2020-01-01&quot;</td><td>1</td><td>&quot;3591759&quot;</td><td>&quot;(2011 YE40)&quot;</td><td>25.2</td><td>0.024241</td><td>0.054205</td><td>24.241248</td><td>54.205079</td><td>0.015063</td><td>0.033681</td><td>79.531656</td><td>177.83819</td><td>false</td><td>false</td><td>&quot;2020-Jan-01 11:55&quot;</td><td>1577879700000</td><td>12.780287</td><td>46009.033071</td><td>28588.22391</td><td>0.061832</td><td>24.052806</td><td>9.2500e6</td><td>5.7477e6</td><td>&quot;Earth&quot;</td><td>0.039223</td><td>0.029964</td><td>0.000032</td><td>0.001208</td><td>0.763932</td><td>&quot;tiny&quot;</td><td>0.005161</td><td>0.000404</td><td>206.692375</td><td>1.381653</td><td>&quot;medium&quot;</td><td>0.000063</td><td>1451.890804</td><td>16.172749</td><td>8.107129</td><td>0.083463</td><td>2.7166e-8</td><td>&quot;close&quot;</td><td>2020-01-01 11:55:00</td><td>2020</td><td>1</td><td>1</td><td>11</td><td>3</td><td>1</td><td>0.5</td><td>0.866025</td><td>0.258819</td><td>-0.965926</td><td>642.477502</td><td>31649.986408</td><td>&quot;very_dim&quot;</td><td>0.501283</td><td>0.019662</td><td>1.141097</td><td>7.978101</td><td>0.357143</td><td>0.547619</td><td>0.214286</td><td>-0.478038</td><td>-0.023414</td><td>-0.986705</td><td>0.038473</td><td>2.623239</td><td>16.040134</td></tr><tr><td>&quot;2020-01-01&quot;</td><td>2</td><td>&quot;3630817&quot;</td><td>&quot;(2013 EC20)&quot;</td><td>29.0</td><td>0.004213</td><td>0.00942</td><td>4.212646</td><td>9.419763</td><td>0.002618</td><td>0.005853</td><td>13.821018</td><td>30.904735</td><td>false</td><td>true</td><td>&quot;2020-Jan-01 03:23&quot;</td><td>1577848980000</td><td>2.793701</td><td>10057.324955</td><td>6249.230609</td><td>0.162019</td><td>63.025272</td><td>2.4238e7</td><td>1.5061e7</td><td>&quot;Earth&quot;</td><td>0.006816</td><td>0.005207</td><td>1.6582e-7</td><td>0.000036</td><td>0.763932</td><td>&quot;tiny&quot;</td><td>0.000001</td><td>4.6324e-7</td><td>17.243081</td><td>0.115263</td><td>&quot;slow&quot;</td><td>0.000164</td><td>3804.37159</td><td>6.172127</td><td>0.117532</td><td>0.000008</td><td>1.4961e-11</td><td>&quot;moderate&quot;</td><td>2020-01-01 03:23:00</td><td>2020</td><td>1</td><td>1</td><td>3</td><td>3</td><td>1</td><td>0.5</td><td>0.866025</td><td>0.707107</td><td>0.707107</td><td>4254.567124</td><td>6.0308e6</td><td>&quot;very_dim&quot;</td><td>0.019042</td><td>0.00013</td><td>0.249438</td><td>0.116811</td><td>0.02381</td><td>0.02381</td><td>0.468254</td><td>-0.641399</td><td>-1.409659</td><td>-0.301277</td><td>0.006793</td><td>1.333342</td><td>17.003418</td></tr><tr><td>&quot;2020-01-01&quot;</td><td>3</td><td>&quot;3747497&quot;</td><td>&quot;(2016 EF195)&quot;</td><td>25.5</td><td>0.021113</td><td>0.047211</td><td>21.113244</td><td>47.21065</td><td>0.013119</td><td>0.029335</td><td>69.269177</td><td>154.890589</td><td>false</td><td>false</td><td>&quot;2020-Jan-01 08:44&quot;</td><td>1577868240000</td><td>17.548446</td><td>63174.405279</td><td>39254.118658</td><td>0.276326</td><td>107.490916</td><td>4.1338e7</td><td>2.5686e7</td><td>&quot;Earth&quot;</td><td>0.034162</td><td>0.026097</td><td>0.000021</td><td>0.000917</td><td>0.763932</td><td>&quot;tiny&quot;</td><td>0.006428</td><td>0.000366</td><td>63.506254</td><td>0.424513</td><td>&quot;medium&quot;</td><td>0.00028</td><td>6488.435131</td><td>3.61891</td><td>2.169497</td><td>0.023264</td><td>8.6939e-9</td><td>&quot;far&quot;</td><td>2020-01-01 08:44:00</td><td>2020</td><td>1</td><td>1</td><td>8</td><td>3</td><td>1</td><td>0.5</td><td>0.866025</td><td>0.866025</td><td>-0.5</td><td>746.44457</td><td>47904.192775</td><td>&quot;very_dim&quot;</td><td>0.599489</td><td>0.02048</td><td>1.566826</td><td>2.161674</td><td>0.293651</td><td>0.777778</td><td>0.65873</td><td>-0.503551</td><td>0.638457</td><td>0.480761</td><td>0.033591</td><td>2.920386</td><td>17.537288</td></tr><tr><td>&quot;2020-01-01&quot;</td><td>4</td><td>&quot;3893737&quot;</td><td>&quot;(2019 WE5)&quot;</td><td>23.3</td><td>0.058151</td><td>0.130029</td><td>58.150704</td><td>130.028927</td><td>0.036133</td><td>0.080796</td><td>190.783156</td><td>426.604105</td><td>false</td><td>false</td><td>&quot;2020-Jan-01 14:55&quot;</td><td>1577890500000</td><td>5.002825</td><td>18010.17068</td><td>11190.819665</td><td>0.134597</td><td>52.3584</td><td>2.0135e7</td><td>1.2512e7</td><td>&quot;Earth&quot;</td><td>0.09409</td><td>0.071878</td><td>0.000436</td><td>0.006953</td><td>0.763932</td><td>&quot;small&quot;</td><td>0.010916</td><td>0.002182</td><td>37.168802</td><td>0.248458</td><td>&quot;slow&quot;</td><td>0.000136</td><td>3160.491096</td><td>7.429562</td><td>3.497206</td><td>0.0811</td><td>1.1004e-8</td><td>&quot;moderate&quot;</td><td>2020-01-01 14:55:00</td><td>2020</td><td>1</td><td>1</td><td>14</td><td>3</td><td>1</td><td>0.5</td><td>0.866025</td><td>-0.5</td><td>-0.866025</td><td>247.635728</td><td>2292.838824</td><td>&quot;dim&quot;</td><td>0.470715</td><td>0.044289</td><td>0.446681</td><td>3.471415</td><td>0.59127</td><td>0.095238</td><td>0.420635</td><td>-0.20146</td><td>-1.103009</td><td>-0.488881</td><td>0.089923</td><td>1.79223</td><td>16.817994</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 70)\n",
       "┌────────────┬────────────┬─────────┬────────┬───┬────────────┬────────────┬───────────┬───────────┐\n",
       "│ date       ┆ obj_that_d ┆ id      ┆ name   ┆ … ┆ distance_z ┆ log_diamet ┆ log_veloc ┆ log_dista │\n",
       "│ ---        ┆ ay         ┆ ---     ┆ ---    ┆   ┆ score      ┆ er         ┆ ity       ┆ nce       │\n",
       "│ str        ┆ ---        ┆ str     ┆ str    ┆   ┆ ---        ┆ ---        ┆ ---       ┆ ---       │\n",
       "│            ┆ i64        ┆         ┆        ┆   ┆ f64        ┆ f64        ┆ f64       ┆ f64       │\n",
       "╞════════════╪════════════╪═════════╪════════╪═══╪════════════╪════════════╪═══════════╪═══════════╡\n",
       "│ 2020-01-01 ┆ 0          ┆ 3564720 ┆ (2011  ┆ … ┆ -0.049103  ┆ 0.208663   ┆ 2.932496  ┆ 17.208398 │\n",
       "│            ┆            ┆         ┆ HS60)  ┆   ┆            ┆            ┆           ┆           │\n",
       "│ 2020-01-01 ┆ 1          ┆ 3591759 ┆ (2011  ┆ … ┆ -0.986705  ┆ 0.038473   ┆ 2.623239  ┆ 16.040134 │\n",
       "│            ┆            ┆         ┆ YE40)  ┆   ┆            ┆            ┆           ┆           │\n",
       "│ 2020-01-01 ┆ 2          ┆ 3630817 ┆ (2013  ┆ … ┆ -0.301277  ┆ 0.006793   ┆ 1.333342  ┆ 17.003418 │\n",
       "│            ┆            ┆         ┆ EC20)  ┆   ┆            ┆            ┆           ┆           │\n",
       "│ 2020-01-01 ┆ 3          ┆ 3747497 ┆ (2016  ┆ … ┆ 0.480761   ┆ 0.033591   ┆ 2.920386  ┆ 17.537288 │\n",
       "│            ┆            ┆         ┆ EF195) ┆   ┆            ┆            ┆           ┆           │\n",
       "│ 2020-01-01 ┆ 4          ┆ 3893737 ┆ (2019  ┆ … ┆ -0.488881  ┆ 0.089923   ┆ 1.79223   ┆ 16.817994 │\n",
       "│            ┆            ┆         ┆ WE5)   ┆   ┆            ┆            ┆           ┆           │\n",
       "└────────────┴────────────┴─────────┴────────┴───┴────────────┴────────────┴───────────┴───────────┘"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62eeba11",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data.write_csv('feature_engineered_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce57a2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== MISSING VALUES ===\")\n",
    "null_counts = new_data.null_count()\n",
    "print(\"Columns with missing values:\")\n",
    "null_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef17931",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== DATA SHAPE & TYPES ===\")\n",
    "print(f\"Shape: {new_data.shape}\")\n",
    "print(f\"Columns: {len(new_data.columns)}\")\n",
    "print(\"\\nData types:\")\n",
    "print(new_data.dtypes)\n",
    "\n",
    "print(\"\\n=== TARGET VARIABLE ===\")\n",
    "print(f\"Target distribution:\")\n",
    "print(new_data['is_potentially_hazardous'].value_counts())\n",
    "\n",
    "print(\"\\n=== MISSING VALUES ===\")\n",
    "null_counts = new_data.null_count().to_series().to_list()\n",
    "print(\"Columns with missing values:\")\n",
    "print(null_counts)\n",
    "\n",
    "print(\"\\n=== SAMPLE ROWS ===\")\n",
    "print(new_data.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "750b867c",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols = new_data.select(pl.col(pl.Float64, pl.Int64, pl.Int32, pl.Int8, pl.Int16)).columns\n",
    "numerical_cols = [col for col in numerical_cols if col not in ['obj_that_day', 'epoch_date_close_approach']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c1de3a72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['absolute_magniutude_h',\n",
       " 'estimated_diameter_min_km',\n",
       " 'estimated_diameter_max_km',\n",
       " 'estimated_diameter_min_m',\n",
       " 'estimated_diameter_max_m',\n",
       " 'estimated_diameter_min_miles',\n",
       " 'estimated_diameter_max_miles',\n",
       " 'estimated_diameter_min_feet',\n",
       " 'estimated_diameter_max_feet',\n",
       " 'relative_velocity_km/sec',\n",
       " 'relative_velocity_km/hr',\n",
       " 'relative_velocity_mph',\n",
       " 'miss_distance_astronomical',\n",
       " 'miss_distance_lunar',\n",
       " 'miss_distance_kilometers',\n",
       " 'miss_distance_miles',\n",
       " 'avg_diameter_km',\n",
       " 'diameter_uncertainty_km',\n",
       " 'estimated_volume',\n",
       " 'cross_section_area_km2',\n",
       " 'diameter_uncertainty_ratio',\n",
       " 'kenetic_energy',\n",
       " 'momentum',\n",
       " 'velocity_per_au',\n",
       " 'velocity_distance_ratio',\n",
       " 'lunar_distance_ratio',\n",
       " 'earth_radii_distance',\n",
       " 'close_approach_score',\n",
       " 'impact_potential',\n",
       " 'destruction_potential',\n",
       " 'hazard_index',\n",
       " 'approach_year',\n",
       " 'approach_month',\n",
       " 'approach_day',\n",
       " 'approach_hour',\n",
       " 'day_of_week',\n",
       " 'day_of_year',\n",
       " 'month_sin',\n",
       " 'month_cos',\n",
       " 'hour_sin',\n",
       " 'hour_cos',\n",
       " 'brightness_size_ratio',\n",
       " 'apparent_density_inverse',\n",
       " 'size_velocity_product',\n",
       " 'size_squared_velocity',\n",
       " 'escape_velocity_ratio',\n",
       " 'threat_score',\n",
       " 'size_percentile',\n",
       " 'velocity_percentile',\n",
       " 'distance_percentile',\n",
       " 'size_zscore',\n",
       " 'velocity_zscore',\n",
       " 'distance_zscore',\n",
       " 'log_diameter',\n",
       " 'log_velocity',\n",
       " 'log_distance']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74696e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection_corr(data):\n",
    "    \n",
    "\n",
    "    correlations = []\n",
    "    for col in numerical_cols:\n",
    "        try:\n",
    "            corr = data.select(pl.corr('is_potentially_hazardous', col)).item()\n",
    "            if corr is not None:\n",
    "                correlations.append((col, abs(col)))\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    correlations.sort(key=lambda x: x[1], reverse=True)\n",
    "    return correlations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7970bc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations = feature_selection_corr(new_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fcec78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "temp_data = new_data.select(numerical_cols)\n",
    "\n",
    "correlations = {}\n",
    "for col in numerical_cols:\n",
    "    corr = new_data.select(pl.corr('is_potentially_hazardous', col)).item()\n",
    "    correlations[col] = corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fe264443",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "658ec597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'date': String\n",
      "Column 'obj_that_day': Int64\n",
      "Column 'id': String\n",
      "Column 'name': String\n",
      "Column 'absolute_magniutude_h': Float64\n",
      "Column 'estimated_diameter_min_km': Float64\n",
      "Column 'estimated_diameter_max_km': Float64\n",
      "Column 'estimated_diameter_min_m': Float64\n",
      "Column 'estimated_diameter_max_m': Float64\n",
      "Column 'estimated_diameter_min_miles': Float64\n",
      "Column 'estimated_diameter_max_miles': Float64\n",
      "Column 'estimated_diameter_min_feet': Float64\n",
      "Column 'estimated_diameter_max_feet': Float64\n",
      "Column 'is_potentially_hazardous': Boolean\n",
      "Column 'is_sentry_object': Boolean\n",
      "Column 'close_approach_date': String\n",
      "Column 'epoch_date_close_approach': Int64\n",
      "Column 'relative_velocity_km/sec': Float64\n",
      "Column 'relative_velocity_km/hr': Float64\n",
      "Column 'relative_velocity_mph': Float64\n",
      "Column 'miss_distance_astronomical': Float64\n",
      "Column 'miss_distance_lunar': Float64\n",
      "Column 'miss_distance_kilometers': Float64\n",
      "Column 'miss_distance_miles': Float64\n",
      "Column 'oribiting_body': String\n",
      "Column 'avg_diameter_km': Float64\n",
      "Column 'diameter_uncertainty_km': Float64\n",
      "Column 'estimated_volume': Float64\n",
      "Column 'cross_section_area_km2': Float64\n",
      "Column 'diameter_uncertainty_ratio': Float64\n",
      "Column 'size_category': String\n",
      "Column 'kenetic_energy': Float64\n",
      "Column 'momentum': Float64\n",
      "Column 'velocity_per_au': Float64\n",
      "Column 'velocity_distance_ratio': Float64\n",
      "Column 'velocity_category': String\n",
      "Column 'lunar_distance_ratio': Float64\n",
      "Column 'earth_radii_distance': Float64\n",
      "Column 'close_approach_score': Float64\n",
      "Column 'impact_potential': Float64\n",
      "Column 'destruction_potential': Float64\n",
      "Column 'hazard_index': Float64\n",
      "Column 'proximity_level': String\n",
      "Column 'approach_datetime': Datetime(time_unit='ms', time_zone=None)\n",
      "Column 'approach_year': Int32\n",
      "Column 'approach_month': Int8\n",
      "Column 'approach_day': Int8\n",
      "Column 'approach_hour': Int8\n",
      "Column 'day_of_week': Int8\n",
      "Column 'day_of_year': Int16\n",
      "Column 'month_sin': Float64\n",
      "Column 'month_cos': Float64\n",
      "Column 'hour_sin': Float64\n",
      "Column 'hour_cos': Float64\n",
      "Column 'brightness_size_ratio': Float64\n",
      "Column 'apparent_density_inverse': Float64\n",
      "Column 'brightness_category': String\n",
      "Column 'size_velocity_product': Float64\n",
      "Column 'size_squared_velocity': Float64\n",
      "Column 'escape_velocity_ratio': Float64\n",
      "Column 'threat_score': Float64\n",
      "Column 'size_percentile': Float64\n",
      "Column 'velocity_percentile': Float64\n",
      "Column 'distance_percentile': Float64\n",
      "Column 'size_zscore': Float64\n",
      "Column 'velocity_zscore': Float64\n",
      "Column 'distance_zscore': Float64\n",
      "Column 'log_diameter': Float64\n",
      "Column 'log_velocity': Float64\n",
      "Column 'log_distance': Float64\n",
      "\n",
      "📊 Selected numerical features: 55\n"
     ]
    }
   ],
   "source": [
    "# Put all imports at the top\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "def get_all_numerical_features_simple(data):\n",
    "    \"\"\"\n",
    "    Simple approach to get numerical features\n",
    "    \"\"\"\n",
    "    numerical_features = []\n",
    "    \n",
    "    for col in data.columns:\n",
    "        dtype = str(data[col].dtype)\n",
    "        print(f\"Column '{col}': {dtype}\")\n",
    "        \n",
    "        # Check if it's a numerical type\n",
    "        if any(num_type in dtype.lower() for num_type in ['int', 'float']):\n",
    "            if col not in ['obj_that_day', 'epoch_date_close_approach', 'approach_year']:\n",
    "                numerical_features.append(col)\n",
    "    \n",
    "    print(f\"\\n📊 Selected numerical features: {len(numerical_features)}\")\n",
    "    return numerical_features\n",
    "\n",
    "# Try this simpler approach\n",
    "selected_features = get_all_numerical_features_simple(new_data)\n",
    "\n",
    "def prepare_data_splits(data, selected_features):\n",
    "    X = data.select(selected_features).to_pandas()\n",
    "    y = data.select('is_potentially_hazardous').to_pandas().squeeze()\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.25, stratify=y, random_state=42\n",
    "    )\n",
    "    \n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    return X_train, X_test, y_train, y_test, cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d9c2673d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'date': String\n",
      "Column 'obj_that_day': Int64\n",
      "Column 'id': String\n",
      "Column 'name': String\n",
      "Column 'absolute_magniutude_h': Float64\n",
      "Column 'estimated_diameter_min_km': Float64\n",
      "Column 'estimated_diameter_max_km': Float64\n",
      "Column 'estimated_diameter_min_m': Float64\n",
      "Column 'estimated_diameter_max_m': Float64\n",
      "Column 'estimated_diameter_min_miles': Float64\n",
      "Column 'estimated_diameter_max_miles': Float64\n",
      "Column 'estimated_diameter_min_feet': Float64\n",
      "Column 'estimated_diameter_max_feet': Float64\n",
      "Column 'is_potentially_hazardous': Boolean\n",
      "Column 'is_sentry_object': Boolean\n",
      "Column 'close_approach_date': String\n",
      "Column 'epoch_date_close_approach': Int64\n",
      "Column 'relative_velocity_km/sec': Float64\n",
      "Column 'relative_velocity_km/hr': Float64\n",
      "Column 'relative_velocity_mph': Float64\n",
      "Column 'miss_distance_astronomical': Float64\n",
      "Column 'miss_distance_lunar': Float64\n",
      "Column 'miss_distance_kilometers': Float64\n",
      "Column 'miss_distance_miles': Float64\n",
      "Column 'oribiting_body': String\n",
      "Column 'avg_diameter_km': Float64\n",
      "Column 'diameter_uncertainty_km': Float64\n",
      "Column 'estimated_volume': Float64\n",
      "Column 'cross_section_area_km2': Float64\n",
      "Column 'diameter_uncertainty_ratio': Float64\n",
      "Column 'size_category': String\n",
      "Column 'kenetic_energy': Float64\n",
      "Column 'momentum': Float64\n",
      "Column 'velocity_per_au': Float64\n",
      "Column 'velocity_distance_ratio': Float64\n",
      "Column 'velocity_category': String\n",
      "Column 'lunar_distance_ratio': Float64\n",
      "Column 'earth_radii_distance': Float64\n",
      "Column 'close_approach_score': Float64\n",
      "Column 'impact_potential': Float64\n",
      "Column 'destruction_potential': Float64\n",
      "Column 'hazard_index': Float64\n",
      "Column 'proximity_level': String\n",
      "Column 'approach_datetime': Datetime(time_unit='ms', time_zone=None)\n",
      "Column 'approach_year': Int32\n",
      "Column 'approach_month': Int8\n",
      "Column 'approach_day': Int8\n",
      "Column 'approach_hour': Int8\n",
      "Column 'day_of_week': Int8\n",
      "Column 'day_of_year': Int16\n",
      "Column 'month_sin': Float64\n",
      "Column 'month_cos': Float64\n",
      "Column 'hour_sin': Float64\n",
      "Column 'hour_cos': Float64\n",
      "Column 'brightness_size_ratio': Float64\n",
      "Column 'apparent_density_inverse': Float64\n",
      "Column 'brightness_category': String\n",
      "Column 'size_velocity_product': Float64\n",
      "Column 'size_squared_velocity': Float64\n",
      "Column 'escape_velocity_ratio': Float64\n",
      "Column 'threat_score': Float64\n",
      "Column 'size_percentile': Float64\n",
      "Column 'velocity_percentile': Float64\n",
      "Column 'distance_percentile': Float64\n",
      "Column 'size_zscore': Float64\n",
      "Column 'velocity_zscore': Float64\n",
      "Column 'distance_zscore': Float64\n",
      "Column 'log_diameter': Float64\n",
      "Column 'log_velocity': Float64\n",
      "Column 'log_distance': Float64\n",
      "\n",
      "📊 Selected numerical features: 55\n"
     ]
    }
   ],
   "source": [
    "selected_features = get_all_numerical_features_simple(new_data)\n",
    "X_train, X_test, y_train, y_test, cv = prepare_data_splits(new_data, selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "aa95ceb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "def smart_preprocessing(X_train, X_test, selected_features):\n",
    "    \"\"\"\n",
    "    Smart preprocessing that handles only the features that actually exist\n",
    "    \"\"\"\n",
    "    \n",
    "    # Debug: Check what columns we actually have\n",
    "    print(\"🔍 Available columns in X_train:\")\n",
    "    print(X_train.columns.tolist())\n",
    "    print(f\"\\n🔍 Selected features we're looking for:\")\n",
    "    print(selected_features)\n",
    "    \n",
    "    # Find boolean features that actually exist in the data\n",
    "    potential_boolean_features = ['is_sentry_object']\n",
    "    actual_boolean_features = [col for col in potential_boolean_features if col in X_train.columns]\n",
    "    \n",
    "    # All other features are numerical\n",
    "    numerical_features = [col for col in selected_features if col not in actual_boolean_features]\n",
    "    \n",
    "    print(f\"\\n📊 Feature breakdown:\")\n",
    "    print(f\"Boolean features found: {actual_boolean_features}\")\n",
    "    print(f\"Numerical features: {len(numerical_features)}\")\n",
    "    \n",
    "    # Create appropriate preprocessor based on what we actually have\n",
    "    if actual_boolean_features:\n",
    "        # We have both numerical and boolean features\n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('num', StandardScaler(), numerical_features),\n",
    "                ('bool', 'passthrough', actual_boolean_features)\n",
    "            ],\n",
    "            remainder='drop'\n",
    "        )\n",
    "    else:\n",
    "        # Only numerical features - use simple StandardScaler\n",
    "        print(\"ℹ️  Only numerical features found, using StandardScaler\")\n",
    "        preprocessor = StandardScaler()\n",
    "    \n",
    "    # Fit and transform\n",
    "    X_train_processed = preprocessor.fit_transform(X_train)\n",
    "    X_test_processed = preprocessor.transform(X_test)  # Use transform, not fit_transform!\n",
    "    \n",
    "    print(f\"✅ Preprocessing complete:\")\n",
    "    print(f\"   Training shape: {X_train_processed.shape}\")\n",
    "    print(f\"   Test shape: {X_test_processed.shape}\")\n",
    "    \n",
    "    return X_train_processed, X_test_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "54a26fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Available columns in X_train:\n",
      "['absolute_magniutude_h', 'estimated_diameter_min_km', 'estimated_diameter_max_km', 'estimated_diameter_min_m', 'estimated_diameter_max_m', 'estimated_diameter_min_miles', 'estimated_diameter_max_miles', 'estimated_diameter_min_feet', 'estimated_diameter_max_feet', 'relative_velocity_km/sec', 'relative_velocity_km/hr', 'relative_velocity_mph', 'miss_distance_astronomical', 'miss_distance_lunar', 'miss_distance_kilometers', 'miss_distance_miles', 'avg_diameter_km', 'diameter_uncertainty_km', 'estimated_volume', 'cross_section_area_km2', 'diameter_uncertainty_ratio', 'kenetic_energy', 'momentum', 'velocity_per_au', 'velocity_distance_ratio', 'lunar_distance_ratio', 'earth_radii_distance', 'close_approach_score', 'impact_potential', 'destruction_potential', 'hazard_index', 'approach_month', 'approach_day', 'approach_hour', 'day_of_week', 'day_of_year', 'month_sin', 'month_cos', 'hour_sin', 'hour_cos', 'brightness_size_ratio', 'apparent_density_inverse', 'size_velocity_product', 'size_squared_velocity', 'escape_velocity_ratio', 'threat_score', 'size_percentile', 'velocity_percentile', 'distance_percentile', 'size_zscore', 'velocity_zscore', 'distance_zscore', 'log_diameter', 'log_velocity', 'log_distance']\n",
      "\n",
      "🔍 Selected features we're looking for:\n",
      "['absolute_magniutude_h', 'estimated_diameter_min_km', 'estimated_diameter_max_km', 'estimated_diameter_min_m', 'estimated_diameter_max_m', 'estimated_diameter_min_miles', 'estimated_diameter_max_miles', 'estimated_diameter_min_feet', 'estimated_diameter_max_feet', 'relative_velocity_km/sec', 'relative_velocity_km/hr', 'relative_velocity_mph', 'miss_distance_astronomical', 'miss_distance_lunar', 'miss_distance_kilometers', 'miss_distance_miles', 'avg_diameter_km', 'diameter_uncertainty_km', 'estimated_volume', 'cross_section_area_km2', 'diameter_uncertainty_ratio', 'kenetic_energy', 'momentum', 'velocity_per_au', 'velocity_distance_ratio', 'lunar_distance_ratio', 'earth_radii_distance', 'close_approach_score', 'impact_potential', 'destruction_potential', 'hazard_index', 'approach_month', 'approach_day', 'approach_hour', 'day_of_week', 'day_of_year', 'month_sin', 'month_cos', 'hour_sin', 'hour_cos', 'brightness_size_ratio', 'apparent_density_inverse', 'size_velocity_product', 'size_squared_velocity', 'escape_velocity_ratio', 'threat_score', 'size_percentile', 'velocity_percentile', 'distance_percentile', 'size_zscore', 'velocity_zscore', 'distance_zscore', 'log_diameter', 'log_velocity', 'log_distance']\n",
      "\n",
      "📊 Feature breakdown:\n",
      "Boolean features found: []\n",
      "Numerical features: 55\n",
      "ℹ️  Only numerical features found, using StandardScaler\n",
      "✅ Preprocessing complete:\n",
      "   Training shape: (94, 55)\n",
      "   Test shape: (32, 55)\n"
     ]
    }
   ],
   "source": [
    "X_train_processed, X_test_processed = smart_preprocessing(X_train, X_test, selected_features=selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2e1f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "def baseline_model(X_train_processed, X_test_processed, y_train, y_test, cv):\n",
    "    # Model 1: Random Forest\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=200, # More trees for better performance\n",
    "        class_weight='balanced', # Handle 12 vs 114 imbalance\n",
    "        random_state=42,\n",
    "        max_depth=4, # Shallow to prevent overfitting\n",
    "        min_samples_split=5, # Conservastive splitting\n",
    "        min_samples_leaf=2 # Ensure meaning full leaves\n",
    "    )\n",
    "\n",
    "    # Cross val scores\n",
    "    rf_cv_scores = cross_val_score(rf, X_train_processed, y_train, cv=cv, scoring='f1')\n",
    "    print(f\"F1 Scores: {rf_cv_scores}\")\n",
    "    print(f\"Mean F1: {rf_cv_scores.mean():.3f} (+/- {rf_cv_scores.std() * 2:.3f})\")\n",
    "\n",
    "    # Fit and evaluate\n",
    "    rf.fit(X_train_processed, y_train)\n",
    "    rf_pred = rf.predict(X_test_processed)\n",
    "    rf_prob = rf.predict_proba(X_test_processed)[:, 1]\n",
    "    print(\"-------------RANDOM FOREST TEST RESULTS-------------\")\n",
    "    print(classification_report(y_test, rf_pred))\n",
    "    print(f\"ROC-AUC: {roc_auc_score(y_test, rf_prob):.3f}\")\n",
    "\n",
    "    # Model 2: Logistic Regression\n",
    "    lr = LogisticRegression(\n",
    "        class_weight='balanced',\n",
    "        random_state=42,\n",
    "        max_iter=2000,\n",
    "        C=0.1\n",
    "    )\n",
    "\n",
    "    lr_cv_scores = cross_val_score(lr, X_train_processed, y_train, cv=cv, scoring='f1')\n",
    "    print(f\"F1 Scores: {lr_cv_scores}\")\n",
    "    print(f\"Mean F1: {lr_cv_scores.mean():.3f} (+/- {lr_cv_scores.std() * 2:.3f})\")\n",
    "\n",
    "    lr.fit(X_train_processed, y_train)\n",
    "    lr_pred = lr.predict(X_test_processed)\n",
    "    lr_prob = lr.predict_proba(X_test_processed)[:, 1]\n",
    "    print(\"-------------LOGISITIC REGRESSION TEST RESULTS-------------\")\n",
    "    print(classification_report(y_test, lr_pred))\n",
    "    print(f\"ROC-AUC: {roc_auc_score(y_test, lr_prob):.3f}\")\n",
    "\n",
    "    return rf_pred, rf_prob, lr_pred, lr_prob, rf, lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1e2040",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pred, rf_prob, lr_pred, lr_prob, rf, lr = baseline_model(X_train_processed=X_train_processed, X_test_processed=X_test_processed, y_train=y_train, y_test=y_test, cv=cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f85912",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def feature_importance(model):\n",
    "    feature_names = [col for col in selected_features]\n",
    "    importances = model.feature_importances_\n",
    "\n",
    "    feature_importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': importances\n",
    "    }).sort_values('importance', ascending=False)\n",
    "\n",
    "    return feature_importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2da5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance(rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab371137",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdd877d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "def xgb_model():\n",
    "    # XGBBoost\n",
    "    xgb_model = xgb.XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=3,\n",
    "        learning_rate=0.1,\n",
    "        scale_pos_weight=len(y_train[y_train == False]) / len(y_train[y_train == False]),\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    xgb_cv_scores = cross_val_score(xgb_model, X_train_processed, y_train, cv=cv, scoring='f1')\n",
    "    print(\"🚀 XGBOOST CROSS-VALIDATION:\")\n",
    "    print(f\"Mean F1: {xgb_cv_scores.mean():.3f} (+/- {xgb_cv_scores.std() * 2:.3f})\")\n",
    "\n",
    "    # Test performance\n",
    "    xgb_model.fit(X_train_processed, y_train)\n",
    "    xgb_pred = xgb_model.predict(X_test_processed)\n",
    "    xgb_prob = xgb_model.predict_proba(X_test_processed)[:, 1]\n",
    "\n",
    "    print(\"\\n🚀 XGBOOST TEST RESULTS:\")\n",
    "    print(classification_report(y_test, xgb_pred))\n",
    "    print(f\"ROC-AUC: {roc_auc_score(y_test, xgb_prob):.3f}\")\n",
    "\n",
    "    return xgb_pred, xgb_prob, xgb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041cb43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_pred, xgb_prob, xgb_model = xgb_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1db20f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "def svc_model():\n",
    "    svm = SVC(\n",
    "        probability=True, \n",
    "        class_weight='balanced',\n",
    "        kernel='rbf',\n",
    "        C=1.0,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    svm_cross_scores = cross_val_score(svm, X_train_processed, y_train, cv=cv, scoring='f1')\n",
    "    print(\"🚀 SVM CROSS-VALIDATION:\")\n",
    "    print(f\"Mean F1: {svm_cross_scores.mean():.3f} (+/- {svm_cross_scores.std() * 2:.3f})\")\n",
    "\n",
    "    # Test performance\n",
    "    svm.fit(X_train_processed, y_train)\n",
    "    svm_pred = svm.predict(X_test_processed)\n",
    "    svm_prob = svm.predict_proba(X_test_processed)[:, 1]\n",
    "\n",
    "    print(\"\\n🚀 SVM TEST RESULTS:\")\n",
    "    print(classification_report(y_test, svm_pred))\n",
    "    print(f\"ROC-AUC: {roc_auc_score(y_test, svm_prob):.3f}\")\n",
    "\n",
    "    return svm_pred, svm_prob, svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7d71615e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 SVM CROSS-VALIDATION:\n",
      "Mean F1: 0.460 (+/- 0.098)\n",
      "\n",
      "🚀 SVM TEST RESULTS:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       1.00      0.86      0.93        29\n",
      "        True       0.43      1.00      0.60         3\n",
      "\n",
      "    accuracy                           0.88        32\n",
      "   macro avg       0.71      0.93      0.76        32\n",
      "weighted avg       0.95      0.88      0.90        32\n",
      "\n",
      "ROC-AUC: 0.943\n"
     ]
    }
   ],
   "source": [
    "svm_pred, svm_prob, svm = svc_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf611d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_boosting_model():\n",
    "    gb = GradientBoostingClassifier(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=3,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    gb_cross_scores = cross_val_score(gb, X_train_processed, y_train, cv=cv, scoring='f1')\n",
    "    print(\"🚀 Gradient Boosting CROSS-VALIDATION:\")\n",
    "    print(f\"Mean F1: {gb_cross_scores.mean():.3f} (+/- {gb_cross_scores.std() * 2:.3f})\")\n",
    "\n",
    "    # Test performance\n",
    "    gb.fit(X_train_processed, y_train)\n",
    "    gb_pred = gb.predict(X_test_processed)\n",
    "    gb_prob = gb.predict_proba(X_test_processed)[:, 1]\n",
    "\n",
    "    print(\"\\n🚀 Gradient Boosting TEST RESULTS:\")\n",
    "    print(classification_report(y_test, gb_pred))\n",
    "    print(f\"ROC-AUC: {roc_auc_score(y_test, gb_prob):.3f}\")\n",
    "\n",
    "    return gb_pred, gb_prob, gb\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e46d757",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_pred, gb_prob, gb = gradient_boosting_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fe90c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "def basic_neural_net():\n",
    "    mlp = MLPClassifier(\n",
    "        hidden_layer_sizes=(20,10),\n",
    "        max_iter=1000,\n",
    "        random_state=42,\n",
    "        early_stopping=True,\n",
    "        validation_fraction=0.2\n",
    "    )\n",
    "\n",
    "    mlp_cross_scores = cross_val_score(mlp, X_train_processed, y_train, cv=cv, scoring='f1')\n",
    "    print(\"🚀 Basic Neural Net CROSS-VALIDATION:\")\n",
    "    print(f\"Mean F1: {mlp_cross_scores.mean():.3f} (+/- {mlp_cross_scores.std() * 2:.3f})\")\n",
    "\n",
    "    # Test performance\n",
    "    mlp.fit(X_train_processed, y_train)\n",
    "    mlp_pred = mlp.predict(X_test_processed)\n",
    "    mlp_prob = mlp.predict_proba(X_test_processed)[:, 1]\n",
    "\n",
    "    print(\"\\n🚀 Basic Neural Net TEST RESULTS:\")\n",
    "    print(classification_report(y_test, mlp_pred))\n",
    "    print(f\"ROC-AUC: {roc_auc_score(y_test, mlp_prob):.3f}\")\n",
    "\n",
    "    return mlp_pred, mlp_prob, mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41528cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_pred, mlp_prob, mlp = basic_neural_net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1e46d264",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "def tune_svv():\n",
    "    svm_param_grid = {\n",
    "        'C': [0.1, 1, 10, 100],\n",
    "        'gamma': ['scale', 'auto', 0.01, 0.1, 1],\n",
    "        'kernel': ['rbf', 'poly'],\n",
    "        'class_weight': ['blanced', {False: 1, True: 5}, {False: 1, True: 10}]\n",
    "    }\n",
    "\n",
    "    svm_grid = GridSearchCV(\n",
    "        svm,\n",
    "        svm_param_grid,\n",
    "        cv=cv,\n",
    "        scoring='f1',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    print(f\"Tuning SVM\")\n",
    "    svm_grid.fit(X_train_processed, y_train)\n",
    "\n",
    "    print(\"🎯 BEST SVM PARAMETERS:\")\n",
    "    print(svm_grid.best_params_)\n",
    "    print(f\"Best CV F1: {svm_grid.best_score_:.3f}\")\n",
    "\n",
    "    best_svm = svm_grid.best_estimator_\n",
    "    best_svm_pred = best_svm.predict(X_test_processed)\n",
    "    best_svm_prob = best_svm.predict_proba(X_test_processed)[:, 1]\n",
    "\n",
    "    print('OPTIMISED SVM RESULTS')\n",
    "    print(classification_report(y_test, best_svm_pred))\n",
    "    print(f\"ROC-AUC: {roc_auc_score(y_test, best_svm_prob):.3f}\")\n",
    "\n",
    "    return best_svm_pred, best_svm_prob, best_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4886e90f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning SVM\n",
      "🎯 BEST SVM PARAMETERS:\n",
      "{'C': 0.1, 'class_weight': {False: 1, True: 10}, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "Best CV F1: 0.541\n",
      "OPTIMISED SVM RESULTS\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       1.00      0.83      0.91        29\n",
      "        True       0.38      1.00      0.55         3\n",
      "\n",
      "    accuracy                           0.84        32\n",
      "   macro avg       0.69      0.91      0.73        32\n",
      "weighted avg       0.94      0.84      0.87        32\n",
      "\n",
      "ROC-AUC: 0.943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python313\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:516: FitFailedWarning: \n",
      "200 fits failed out of a total of 600.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "200 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Python313\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 859, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python313\\Lib\\site-packages\\sklearn\\base.py\", line 1356, in wrapper\n",
      "    estimator._validate_params()\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"c:\\Python313\\Lib\\site-packages\\sklearn\\base.py\", line 469, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        self._parameter_constraints,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        self.get_params(deep=False),\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        caller_name=self.__class__.__name__,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"c:\\Python313\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 98, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "    ...<2 lines>...\n",
      "    )\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'class_weight' parameter of SVC must be a str among {'balanced'}, an instance of 'dict' or None. Got 'blanced' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Python313\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1135: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan 0.23333333 0.\n",
      " 0.23333333 0.         0.         0.         0.         0.26\n",
      " 0.         0.16       0.51555556 0.21333333 0.51555556 0.12666667\n",
      " 0.54095238 0.05914787 0.23333333 0.31333333 0.18569758 0.16\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan 0.37428571 0.21333333\n",
      " 0.37428571 0.08       0.37428571 0.         0.2        0.16\n",
      " 0.         0.16       0.47428571 0.31333333 0.47428571 0.29333333\n",
      " 0.46       0.16410256 0.2        0.16       0.         0.16\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan 0.24       0.28\n",
      " 0.24       0.33333333 0.19428571 0.31333333 0.2        0.16\n",
      " 0.         0.16       0.24       0.31333333 0.24       0.31333333\n",
      " 0.19428571 0.33333333 0.2        0.16       0.         0.16\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan 0.18       0.16\n",
      " 0.18       0.16       0.16       0.26       0.2        0.16\n",
      " 0.         0.16       0.18       0.16       0.18       0.16\n",
      " 0.16       0.31333333 0.2        0.16       0.         0.16      ]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "svm_pred_new, svm_prob_new, svm_new_model = tune_svv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b4f19e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "def svm_tuning_advanced():\n",
    "    svm_params = {\n",
    "        'C': [0.01, 0.1, 1, 10, 50, 100, 200],\n",
    "        'gamma':  ['scale', 'auto', 0.001, 0.01, 0.1, 1, 10],\n",
    "        'kernel': ['rbf', 'poly', 'sigmoid'],\n",
    "        'class_weight': [\n",
    "            'balanced',\n",
    "            {False: 1, True: 3},\n",
    "            {False: 1, True: 5},\n",
    "            {False: 1, True: 8},\n",
    "            {False: 1, True: 10},\n",
    "            {False: 1, True: 15},\n",
    "            {False: 1, True: 20},\n",
    "        ],\n",
    "        'degree': [2,3,4]\n",
    "    }\n",
    "\n",
    "    svm_random_search = RandomizedSearchCV(\n",
    "        svm_new_model,\n",
    "        svm_params,\n",
    "        n_iter=200,\n",
    "        cv=cv,\n",
    "        scoring='f1',\n",
    "        n_jobs=1,\n",
    "        random_state=42,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    print(f\"Starting SVM random search\")\n",
    "    svm_random_search.fit(X_train_processed, y_train)\n",
    "    print(\"\\n🏆 BEST SVM PARAMETERS:\")\n",
    "    print(svm_random_search.best_params_)\n",
    "    print(f\"Best CV F1: {svm_random_search.best_score_:.3f}\")\n",
    "\n",
    "    # Test best SVM\n",
    "    best_svm_tuned = svm_random_search.best_estimator_\n",
    "    svm_tuned_pred = best_svm_tuned.predict(X_test_processed)\n",
    "    svm_tuned_prob = best_svm_tuned.predict_proba(X_test_processed)[:, 1]\n",
    "\n",
    "    print(\"\\n🎯 OPTIMIZED SVM TEST RESULTS:\")\n",
    "    print(classification_report(y_test, svm_tuned_pred))\n",
    "    print(f\"ROC-AUC: {roc_auc_score(y_test, svm_tuned_prob):.3f}\")\n",
    "\n",
    "    return svm_tuned_pred, svm_tuned_prob, best_svm_tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3bd0dfa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting SVM random search\n",
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n",
      "\n",
      "🏆 BEST SVM PARAMETERS:\n",
      "{'kernel': 'sigmoid', 'gamma': 10, 'degree': 4, 'class_weight': {False: 1, True: 8}, 'C': 0.01}\n",
      "Best CV F1: 0.562\n",
      "\n",
      "🎯 OPTIMIZED SVM TEST RESULTS:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       1.00      0.76      0.86        29\n",
      "        True       0.30      1.00      0.46         3\n",
      "\n",
      "    accuracy                           0.78        32\n",
      "   macro avg       0.65      0.88      0.66        32\n",
      "weighted avg       0.93      0.78      0.83        32\n",
      "\n",
      "ROC-AUC: 0.885\n"
     ]
    }
   ],
   "source": [
    "svm_tuned_pred, svm_tuned_prob, best_svm_tuned = svm_tuning_advanced()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7dda7f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "def svm_threshold_tuning():\n",
    "    svm_probability = best_svm_tuned.predict_proba(X_test_processed)[:, 1]\n",
    "\n",
    "    precision, recall, thresholds = precision_recall_curve(y_test, svm_probability)\n",
    "\n",
    "    target_recall = 0.8\n",
    "    idx = np.argmax(recall >= target_recall)\n",
    "    optimal_threshold = thresholds[idx] if idx < len(thresholds) else 0.5\n",
    "\n",
    "    print(f\"Optimal threshold for {target_recall:.0%} recall: {optimal_threshold:.3f}\")\n",
    "\n",
    "    svm_pred_optimised = (svm_probability >= optimal_threshold).astype(int)\n",
    "    print(classification_report(y_test, svm_pred_optimised))\n",
    "    return svm_pred_optimised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "657750fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal threshold for 80% recall: 0.060\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.00      0.00      0.00        29\n",
      "        True       0.09      1.00      0.17         3\n",
      "\n",
      "    accuracy                           0.09        32\n",
      "   macro avg       0.05      0.50      0.09        32\n",
      "weighted avg       0.01      0.09      0.02        32\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "svm_pred_optimised = svm_threshold_tuning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d5f49aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 PROBABILITY DISTRIBUTION ANALYSIS:\n",
      "Min probability: 0.060\n",
      "Max probability: 0.178\n",
      "Mean probability: 0.108\n",
      "Default threshold (0.5) predictions: 0 hazardous\n",
      "\n",
      "📊 THRESHOLD ANALYSIS:\n",
      "Threshold | Precision | Recall | F1-Score | False Alarms\n",
      "-------------------------------------------------------\n",
      "  0.1     |   0.20    |  1.00  |   0.33   |     12\n",
      "  0.2     |   0.00    |  0.00  |   0.00   |     0\n",
      "  0.3     |   0.00    |  0.00  |   0.00   |     0\n",
      "  0.4     |   0.00    |  0.00  |   0.00   |     0\n",
      "  0.5     |   0.00    |  0.00  |   0.00   |     0\n",
      "  0.6     |   0.00    |  0.00  |   0.00   |     0\n",
      "  0.7     |   0.00    |  0.00  |   0.00   |     0\n",
      "  0.8     |   0.00    |  0.00  |   0.00   |     0\n",
      "  0.9     |   0.00    |  0.00  |   0.00   |     0\n",
      "\n",
      "🏆 BEST THRESHOLD: 0.1 (F1: 0.333)\n",
      "\n",
      "🎯 BEST THRESHOLD RESULTS:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       1.00      0.59      0.74        29\n",
      "        True       0.20      1.00      0.33         3\n",
      "\n",
      "    accuracy                           0.62        32\n",
      "   macro avg       0.60      0.79      0.54        32\n",
      "weighted avg       0.93      0.62      0.70        32\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's do a more systematic threshold analysis\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_curve, f1_score\n",
    "\n",
    "# Get probabilities\n",
    "svm_prob = best_svm_tuned.predict_proba(X_test_processed)[:, 1]\n",
    "\n",
    "print(\"🔍 PROBABILITY DISTRIBUTION ANALYSIS:\")\n",
    "print(f\"Min probability: {svm_prob.min():.3f}\")\n",
    "print(f\"Max probability: {svm_prob.max():.3f}\")\n",
    "print(f\"Mean probability: {svm_prob.mean():.3f}\")\n",
    "print(f\"Default threshold (0.5) predictions: {(svm_prob >= 0.5).sum()} hazardous\")\n",
    "\n",
    "# Test multiple thresholds systematically\n",
    "thresholds_to_test = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "\n",
    "print(\"\\n📊 THRESHOLD ANALYSIS:\")\n",
    "print(\"Threshold | Precision | Recall | F1-Score | False Alarms\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "best_f1 = 0\n",
    "best_threshold = 0.5\n",
    "\n",
    "for thresh in thresholds_to_test:\n",
    "    pred = (svm_prob >= thresh).astype(int)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "    \n",
    "    precision = precision_score(y_test, pred, zero_division=0)\n",
    "    recall = recall_score(y_test, pred, zero_division=0)\n",
    "    f1 = f1_score(y_test, pred, zero_division=0)\n",
    "    false_alarms = ((y_test == False) & (pred == True)).sum()\n",
    "    \n",
    "    print(f\"  {thresh:.1f}     |   {precision:.2f}    |  {recall:.2f}  |   {f1:.2f}   |     {false_alarms}\")\n",
    "    \n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_threshold = thresh\n",
    "\n",
    "print(f\"\\n🏆 BEST THRESHOLD: {best_threshold} (F1: {best_f1:.3f})\")\n",
    "\n",
    "# Apply best threshold\n",
    "best_pred = (svm_prob >= best_threshold).astype(int)\n",
    "print(f\"\\n🎯 BEST THRESHOLD RESULTS:\")\n",
    "print(classification_report(y_test, best_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2558bb14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🌲 RANDOM FOREST RESULTS:\n",
      "Probability range: 0.000 - 0.896\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.94      1.00      0.97        29\n",
      "        True       1.00      0.33      0.50         3\n",
      "\n",
      "    accuracy                           0.94        32\n",
      "   macro avg       0.97      0.67      0.73        32\n",
      "weighted avg       0.94      0.94      0.92        32\n",
      "\n",
      "ROC-AUC: 0.966\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=5,\n",
    "    class_weight='balanced',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "rf_model.fit(X_train_processed, y_train)\n",
    "rf_pred = rf_model.predict(X_test_processed)\n",
    "rf_prob = rf_model.predict_proba(X_test_processed)[:, 1]\n",
    "\n",
    "print(f\"\\n🌲 RANDOM FOREST RESULTS:\")\n",
    "print(f\"Probability range: {rf_prob.min():.3f} - {rf_prob.max():.3f}\")\n",
    "print(classification_report(y_test, rf_pred))\n",
    "print(f\"ROC-AUC: {roc_auc_score(y_test, rf_prob):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e214b22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5bc304",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80af0dd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a65362d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "import pandas as pd\n",
    "\n",
    "def feature_importance_analysis(model):\n",
    "    perm_importance = permutation_importance(\n",
    "        model, X_test_processed, y_test,\n",
    "        n_repeats=20, random_state=42, scoring='f1'\n",
    "    )\n",
    "\n",
    "    feature_importance_df = pd.DataFrame({\n",
    "        'feature': selected_features,\n",
    "        'importance': perm_importance.importances_mean,\n",
    "        'std': perm_importance.importances_std\n",
    "    }).sort_values('importance', ascending=False)\n",
    "\n",
    "    return feature_importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0b0d238e",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance_df = feature_importance_analysis(best_svm_tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "84d585f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>threat_score</td>\n",
       "      <td>0.079563</td>\n",
       "      <td>0.103559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>impact_potential</td>\n",
       "      <td>0.047421</td>\n",
       "      <td>0.088985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>log_velocity</td>\n",
       "      <td>0.039286</td>\n",
       "      <td>0.128940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>log_diameter</td>\n",
       "      <td>0.024603</td>\n",
       "      <td>0.049621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>size_percentile</td>\n",
       "      <td>0.022738</td>\n",
       "      <td>0.250494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>absolute_magniutude_h</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>0.089214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>velocity_percentile</td>\n",
       "      <td>0.009167</td>\n",
       "      <td>0.151441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>avg_diameter_km</td>\n",
       "      <td>0.005556</td>\n",
       "      <td>0.016667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>relative_velocity_km/sec</td>\n",
       "      <td>0.001984</td>\n",
       "      <td>0.023660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>velocity_zscore</td>\n",
       "      <td>0.001984</td>\n",
       "      <td>0.023660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>size_velocity_product</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>brightness_size_ratio</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>is_sentry_object</td>\n",
       "      <td>-0.020119</td>\n",
       "      <td>0.148011</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     feature  importance       std\n",
       "0               threat_score    0.079563  0.103559\n",
       "1           impact_potential    0.047421  0.088985\n",
       "7               log_velocity    0.039286  0.128940\n",
       "3               log_diameter    0.024603  0.049621\n",
       "2            size_percentile    0.022738  0.250494\n",
       "11     absolute_magniutude_h    0.014286  0.089214\n",
       "8        velocity_percentile    0.009167  0.151441\n",
       "4            avg_diameter_km    0.005556  0.016667\n",
       "6   relative_velocity_km/sec    0.001984  0.023660\n",
       "5            velocity_zscore    0.001984  0.023660\n",
       "9      size_velocity_product    0.000000  0.000000\n",
       "12     brightness_size_ratio    0.000000  0.000000\n",
       "10          is_sentry_object   -0.020119  0.148011"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importance_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
