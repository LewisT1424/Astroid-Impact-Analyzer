{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bd360928",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "polars.config.Config"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import polars as pl\n",
    "import json\n",
    "import numpy as np\n",
    "pl.Config.set_fmt_float(\"mixed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db8d60ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/raw/data_2020-01-01.json', 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5727e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_data(data):\n",
    "    data_list = []\n",
    "    for date in data['near_earth_objects']:\n",
    "        result = {'date': date}\n",
    "        for idx, obj in enumerate(data['near_earth_objects'][date]):\n",
    "            new_result = result | {\n",
    "                # General stuff about astroid\n",
    "                'obj_that_day': idx,\n",
    "                'id': str(obj['id']),\n",
    "                'name': obj['name'],\n",
    "                'absolute_magniutude_h': obj['absolute_magnitude_h'],\n",
    "                # Estimated Diameter\n",
    "                'estimated_diameter_min_km': obj['estimated_diameter']['kilometers']['estimated_diameter_min'],\n",
    "                'estimated_diameter_max_km': obj['estimated_diameter']['kilometers']['estimated_diameter_max'],\n",
    "                'estimated_diameter_min_m': obj['estimated_diameter']['meters']['estimated_diameter_min'],\n",
    "                'estimated_diameter_max_m': obj['estimated_diameter']['meters']['estimated_diameter_max'],\n",
    "                'estimated_diameter_min_miles': obj['estimated_diameter']['miles']['estimated_diameter_min'],\n",
    "                'estimated_diameter_max_miles': obj['estimated_diameter']['miles']['estimated_diameter_max'],\n",
    "                'estimated_diameter_min_feet': obj['estimated_diameter']['feet']['estimated_diameter_min'],\n",
    "                'estimated_diameter_max_feet': obj['estimated_diameter']['feet']['estimated_diameter_max'],\n",
    "                # Potentially hazardous, Sentry object refers to if the astroid is tracked by nasa's sentry system \n",
    "                'is_potentially_hazardous': obj['is_potentially_hazardous_asteroid'],\n",
    "                'is_sentry_object': obj['is_sentry_object']\n",
    "            }\n",
    "            for val in obj['close_approach_data']:\n",
    "                final_result = new_result | {\n",
    "                    # Close approaching dates\n",
    "                    'close_approach_date': val['close_approach_date_full'],\n",
    "                    'epoch_date_close_approach': val['epoch_date_close_approach'],\n",
    "                    # Velocity values\n",
    "                    'relative_velocity_km/sec': float(val['relative_velocity']['kilometers_per_second']),\n",
    "                    'relative_velocity_km/hr': float(val['relative_velocity']['kilometers_per_hour']),\n",
    "                    'relative_velocity_mph': float(val['relative_velocity']['miles_per_hour']),\n",
    "                    # Miss distance\n",
    "                    'miss_distance_astronomical': float(val['miss_distance']['astronomical']),\n",
    "                    'miss_distance_lunar': float(val['miss_distance']['lunar']),\n",
    "                    'miss_distance_kilometers': float(val['miss_distance']['kilometers']),\n",
    "                    'miss_distance_miles': float(val['miss_distance']['miles']),\n",
    "                    # Orbiting body \n",
    "                    'oribiting_body': val['orbiting_body']\n",
    "                }\n",
    "\n",
    "                data_list.append(final_result)\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c0f5672e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pl.DataFrame(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a021f297",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['date',\n",
       " 'obj_that_day',\n",
       " 'id',\n",
       " 'name',\n",
       " 'absolute_magniutude_h',\n",
       " 'estimated_diameter_min_km',\n",
       " 'estimated_diameter_max_km',\n",
       " 'estimated_diameter_min_m',\n",
       " 'estimated_diameter_max_m',\n",
       " 'estimated_diameter_min_miles',\n",
       " 'estimated_diameter_max_miles',\n",
       " 'estimated_diameter_min_feet',\n",
       " 'estimated_diameter_max_feet',\n",
       " 'is_potentially_hazardous',\n",
       " 'is_sentry_object',\n",
       " 'close_approach_date',\n",
       " 'epoch_date_close_approach',\n",
       " 'relative_velocity_km/sec',\n",
       " 'relative_velocity_km/hr',\n",
       " 'relative_velocity_mph',\n",
       " 'miss_distance_astronomical',\n",
       " 'miss_distance_lunar',\n",
       " 'miss_distance_kilometers',\n",
       " 'miss_distance_miles',\n",
       " 'oribiting_body']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "33571665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering\n",
    "def feature_engineering(data: pl.DataFrame) -> pl.DataFrame:\n",
    "    df = data.with_columns([\n",
    "        # Size based features\n",
    "        ((pl.col('estimated_diameter_min_km') + pl.col('estimated_diameter_max_km')) / 2).alias('avg_diameter_km'),\n",
    "        (pl.col('estimated_diameter_max_km') - pl.col('estimated_diameter_min_km')).alias('diameter_uncertainty_km'),\n",
    "        \n",
    "        # Volume of sphere (4/3 x pi x r^3)\n",
    "        ((4/3) * np.pi * (((pl.col('estimated_diameter_min_km') + pl.col('estimated_diameter_max_km')) / 4) ** 3)).alias('estimated_volume'),\n",
    "        # Cross section area from earth (pi x r^2)\n",
    "        (np.pi * (((pl.col('estimated_diameter_min_km') + pl.col('estimated_diameter_max_km')) / 4) ** 2)).alias('cross_section_area_km2')\n",
    "    \n",
    "    ]).with_columns([\n",
    "        # Diameter uncertentity ratio \n",
    "        (pl.col('diameter_uncertainty_km') / pl.col('avg_diameter_km')).alias('diameter_uncertainty_ratio'),\n",
    "        # Site category\n",
    "        pl.when(pl.col('avg_diameter_km') < 0.05).then(pl.lit('tiny'))\n",
    "        .when(pl.col('avg_diameter_km') < 0.14).then(pl.lit('small'))\n",
    "        .when(pl.col('avg_diameter_km') < 0.5).then(pl.lit('medium'))\n",
    "        .otherwise(pl.lit('large')).alias('size_category'),\n",
    "\n",
    "        # Velocity & Kenetic energy features\n",
    "        # Kenetic Energy (KE = 1/2 * m * v^2)\n",
    "        (pl.col('estimated_volume') * pl.col('relative_velocity_km/sec') ** 2).alias('kenetic_energy'),\n",
    "        # Momentum (volume x velocity)\n",
    "        (pl.col('estimated_volume') * pl.col('relative_velocity_km/sec')).alias('momentum'),\n",
    "        # velocity per atronomic unit\n",
    "        (pl.col('relative_velocity_km/sec') / pl.col('miss_distance_astronomical')).alias('velocity_per_au'),\n",
    "        # velocity distance ratio (fast + close = dangerous, slow + far = safe)\n",
    "        (pl.col('relative_velocity_km/sec') / (pl.col('miss_distance_kilometers')/ 1e6)).alias('velocity_distance_ratio'),\n",
    "        # Velocity category\n",
    "        pl.when(pl.col('relative_velocity_km/sec') < 10).then(pl.lit('slow'))\n",
    "        .when(pl.col('relative_velocity_km/sec') < 20).then(pl.lit('medium'))\n",
    "        .when(pl.col('relative_velocity_km/sec') < 30).then(pl.lit('fast'))\n",
    "        .otherwise(pl.lit('very_fast')).alias('velocity_category'),\n",
    "\n",
    "    ]).with_columns([\n",
    "        # Distance & Risk factors\n",
    "        # Lunar distance ratio\n",
    "        (pl.col('miss_distance_lunar') / 384400).alias(\"lunar_distance_ratio\"),\n",
    "        # Earth radii distance (earth in radius unites)\n",
    "        (pl.col('miss_distance_kilometers')/ 6371).alias('earth_radii_distance'),\n",
    "        # Close approach score (closer = higher score)\n",
    "        (1 / pl.col('miss_distance_astronomical')).alias('close_approach_score'),\n",
    "        # Impact potential (combined size, speed and proximity threat)\n",
    "        ((pl.col('avg_diameter_km') * pl.col('relative_velocity_km/sec')) / pl.col('miss_distance_astronomical')).alias('impact_potential'),\n",
    "        # Destruction potential (energy density at earths distance)\n",
    "        (pl.col('kenetic_energy') / pl.col('miss_distance_astronomical')).alias('destruction_potential'),\n",
    "        # Hazard index (combined size^2, velocity^2 and distance): (diameter^2 x velocity^2) / distance_km\n",
    "        ((pl.col('avg_diameter_km') ** 2 * pl.col('relative_velocity_km/sec') ** 2) / pl.col('miss_distance_kilometers')).alias('hazard_index'),\n",
    "        # Proximity level\n",
    "        pl.when(pl.col('miss_distance_lunar') < 10).then(pl.lit('very_close'))\n",
    "        .when(pl.col('miss_distance_lunar') < 50).then(pl.lit('close'))\n",
    "        .when(pl.col('miss_distance_lunar') < 100).then(pl.lit('moderate'))\n",
    "        .otherwise(pl.lit('far')).alias('proximity_level'),\n",
    "        \n",
    "        # Approach datetime - Convert epoch timestamp to readable datetime\n",
    "        pl.from_epoch('epoch_date_close_approach', time_unit='ms').alias('approach_datetime')\n",
    "\n",
    "    ]).with_columns([\n",
    "        # Temporal features\n",
    "        # Time components\n",
    "        pl.col('approach_datetime').dt.year().alias('approach_year'),\n",
    "        pl.col('approach_datetime').dt.month().alias('approach_month'),\n",
    "        pl.col('approach_datetime').dt.day().alias('approach_day'),\n",
    "        pl.col('approach_datetime').dt.hour().alias('approach_hour'),\n",
    "\n",
    "        # Day of week and year\n",
    "        pl.col('approach_datetime').dt.weekday().alias('day_of_week'),\n",
    "        pl.col('approach_datetime').dt.ordinal_day().alias('day_of_year')\n",
    "   \n",
    "    ]).with_columns([\n",
    "        # Month_sin/month_cos + hour_sin/hour_cos (cyclical encoding of moinths)\n",
    "        (2 * np.pi * pl.col('approach_month') / 12).sin().alias('month_sin'),\n",
    "        (2 * np.pi * pl.col('approach_month') / 12).cos().alias('month_cos'),\n",
    "        (2 * np.pi * pl.col('approach_hour') / 24).sin().alias('hour_sin'),\n",
    "        (2 * np.pi * pl.col('approach_hour') / 24).cos().alias('hour_cos'),\n",
    "\n",
    "        # Brightness & physical features\n",
    "        # Brightness size ratio\n",
    "        (pl.col('absolute_magniutude_h') / pl.col('avg_diameter_km')).alias('brightness_size_ratio'),\n",
    "        # Apparant densitiy inversed (1/volume) (higher = denser)\n",
    "        # Dense (metal) vs Fluffy (rubble pile)\n",
    "        (1 / pl.col('estimated_volume')).alias('apparent_density_inverse'),\n",
    "\n",
    "        # Brightness category\n",
    "        pl.when(pl.col('absolute_magniutude_h') < 20).then(pl.lit('very_bright'))\n",
    "        .when(pl.col('absolute_magniutude_h') < 22).then(pl.lit('bright'))\n",
    "        .when(pl.col('absolute_magniutude_h') < 25).then(pl.lit('dim'))\n",
    "        .otherwise(pl.lit('very_dim')).alias('brightness_category'),\n",
    "\n",
    "        # Interaction features\n",
    "\n",
    "        # Size velocity produt (size and speed)\n",
    "        (pl.col('avg_diameter_km') * pl.col('relative_velocity_km/sec')).alias(\"size_velocity_product\"),\n",
    "        # Size squared velocity (emphasises on size) (diameter^2 x velocity)\n",
    "        (pl.col('avg_diameter_km') ** 2 * pl.col('relative_velocity_km/sec')).alias(\"size_squared_velocity\"),\n",
    "        # Escape velocity ratio (speed compared to easrth escape velocity)\n",
    "        (pl.col('relative_velocity_km/sec') / 11.2).alias('escape_velocity_ratio'),\n",
    "        # Threat score (size x velocity) / (distance + weighed_value_to_avoid_div_0)\n",
    "        ((pl.col('avg_diameter_km') * pl.col('relative_velocity_km/sec')) / (pl.col('miss_distance_astronomical') + 0.001)).alias('threat_score'),\n",
    "\n",
    "        # Normalisation & Scaling features\n",
    "        # Size percentile (0-1)\n",
    "        (pl.col('avg_diameter_km').rank(method='average') / pl.col('avg_diameter_km').len()).alias('size_percentile'),\n",
    "        # Velocity percentile\n",
    "        (pl.col('relative_velocity_km/sec').rank(method='average') / pl.col('relative_velocity_km/sec').len()).alias('velocity_percentile'),\n",
    "        # Distance percentile\n",
    "        (pl.col('miss_distance_astronomical').rank(method='average') / pl.col('miss_distance_astronomical').len()).alias('distance_percentile')\n",
    "    ]).with_columns([\n",
    "        # Z-scores & Log values\n",
    "        # Size zscore\n",
    "        ((pl.col('avg_diameter_km') - pl.col('avg_diameter_km').mean()) / pl.col('avg_diameter_km').std()).alias('size_zscore'),\n",
    "        # Velocity zscore\n",
    "        ((pl.col('relative_velocity_km/sec') - pl.col('relative_velocity_km/sec').mean()) / pl.col('relative_velocity_km/sec').std()).alias('velocity_zscore'),\n",
    "        # distance zscore\n",
    "        ((pl.col('miss_distance_astronomical') - pl.col('miss_distance_astronomical').mean()) / pl.col('miss_distance_astronomical').std()).alias('distance_zscore'),\n",
    "\n",
    "        # Log diameter \n",
    "        pl.col('avg_diameter_km').log1p().alias('log_diameter'),\n",
    "        # log velocity\n",
    "        pl.col('relative_velocity_km/sec').log1p().alias('log_velocity'),\n",
    "        # log distance\n",
    "        pl.col('miss_distance_kilometers').log1p().alias('log_distance')\n",
    "    ])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "97850477",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = feature_engineering(data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "62eeba11",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data.write_csv('feature_engineered_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ce57a2fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== MISSING VALUES ===\n",
      "Columns with missing values:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (1, 70)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>date</th><th>obj_that_day</th><th>id</th><th>name</th><th>absolute_magniutude_h</th><th>estimated_diameter_min_km</th><th>estimated_diameter_max_km</th><th>estimated_diameter_min_m</th><th>estimated_diameter_max_m</th><th>estimated_diameter_min_miles</th><th>estimated_diameter_max_miles</th><th>estimated_diameter_min_feet</th><th>estimated_diameter_max_feet</th><th>is_potentially_hazardous</th><th>is_sentry_object</th><th>close_approach_date</th><th>epoch_date_close_approach</th><th>relative_velocity_km/sec</th><th>relative_velocity_km/hr</th><th>relative_velocity_mph</th><th>miss_distance_astronomical</th><th>miss_distance_lunar</th><th>miss_distance_kilometers</th><th>miss_distance_miles</th><th>oribiting_body</th><th>avg_diameter_km</th><th>diameter_uncertainty_km</th><th>estimated_volume</th><th>cross_section_area_km2</th><th>diameter_uncertainty_ratio</th><th>size_category</th><th>kenetic_energy</th><th>momentum</th><th>velocity_per_au</th><th>velocity_distance_ratio</th><th>velocity_category</th><th>lunar_distance_ratio</th><th>earth_radii_distance</th><th>close_approach_score</th><th>impact_potential</th><th>destruction_potential</th><th>hazard_index</th><th>proximity_level</th><th>approach_datetime</th><th>approach_year</th><th>approach_month</th><th>approach_day</th><th>approach_hour</th><th>day_of_week</th><th>day_of_year</th><th>month_sin</th><th>month_cos</th><th>hour_sin</th><th>hour_cos</th><th>brightness_size_ratio</th><th>apparent_density_inverse</th><th>brightness_category</th><th>size_velocity_product</th><th>size_squared_velocity</th><th>escape_velocity_ratio</th><th>threat_score</th><th>size_percentile</th><th>velocity_percentile</th><th>distance_percentile</th><th>size_zscore</th><th>velocity_zscore</th><th>distance_zscore</th><th>log_diameter</th><th>log_velocity</th><th>log_distance</th></tr><tr><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td></tr></thead><tbody><tr><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (1, 70)\n",
       "┌──────┬──────────────┬─────┬──────┬───┬──────────────┬──────────────┬──────────────┬──────────────┐\n",
       "│ date ┆ obj_that_day ┆ id  ┆ name ┆ … ┆ distance_zsc ┆ log_diameter ┆ log_velocity ┆ log_distance │\n",
       "│ ---  ┆ ---          ┆ --- ┆ ---  ┆   ┆ ore          ┆ ---          ┆ ---          ┆ ---          │\n",
       "│ u32  ┆ u32          ┆ u32 ┆ u32  ┆   ┆ ---          ┆ u32          ┆ u32          ┆ u32          │\n",
       "│      ┆              ┆     ┆      ┆   ┆ u32          ┆              ┆              ┆              │\n",
       "╞══════╪══════════════╪═════╪══════╪═══╪══════════════╪══════════════╪══════════════╪══════════════╡\n",
       "│ 0    ┆ 0            ┆ 0   ┆ 0    ┆ … ┆ 0            ┆ 0            ┆ 0            ┆ 0            │\n",
       "└──────┴──────────────┴─────┴──────┴───┴──────────────┴──────────────┴──────────────┴──────────────┘"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\n=== MISSING VALUES ===\")\n",
    "null_counts = new_data.null_count()\n",
    "print(\"Columns with missing values:\")\n",
    "null_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6ef17931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DATA SHAPE & TYPES ===\n",
      "Shape: (126, 70)\n",
      "Columns: 70\n",
      "\n",
      "Data types:\n",
      "[String, Int64, String, String, Float64, Float64, Float64, Float64, Float64, Float64, Float64, Float64, Float64, Boolean, Boolean, String, Int64, Float64, Float64, Float64, Float64, Float64, Float64, Float64, String, Float64, Float64, Float64, Float64, Float64, String, Float64, Float64, Float64, Float64, String, Float64, Float64, Float64, Float64, Float64, Float64, String, Datetime(time_unit='ms', time_zone=None), Int32, Int8, Int8, Int8, Int8, Int16, Float64, Float64, Float64, Float64, Float64, Float64, String, Float64, Float64, Float64, Float64, Float64, Float64, Float64, Float64, Float64, Float64, Float64, Float64, Float64]\n",
      "\n",
      "=== TARGET VARIABLE ===\n",
      "Target distribution:\n",
      "shape: (2, 2)\n",
      "┌──────────────────────────┬───────┐\n",
      "│ is_potentially_hazardous ┆ count │\n",
      "│ ---                      ┆ ---   │\n",
      "│ bool                     ┆ u32   │\n",
      "╞══════════════════════════╪═══════╡\n",
      "│ true                     ┆ 12    │\n",
      "│ false                    ┆ 114   │\n",
      "└──────────────────────────┴───────┘\n",
      "\n",
      "=== MISSING VALUES ===\n",
      "Columns with missing values:\n",
      "[0]\n",
      "\n",
      "=== SAMPLE ROWS ===\n",
      "shape: (3, 70)\n",
      "┌────────────┬────────────┬─────────┬───────┬───┬────────────┬────────────┬────────────┬───────────┐\n",
      "│ date       ┆ obj_that_d ┆ id      ┆ name  ┆ … ┆ distance_z ┆ log_diamet ┆ log_veloci ┆ log_dista │\n",
      "│ ---        ┆ ay         ┆ ---     ┆ ---   ┆   ┆ score      ┆ er         ┆ ty         ┆ nce       │\n",
      "│ str        ┆ ---        ┆ str     ┆ str   ┆   ┆ ---        ┆ ---        ┆ ---        ┆ ---       │\n",
      "│            ┆ i64        ┆         ┆       ┆   ┆ f64        ┆ f64        ┆ f64        ┆ f64       │\n",
      "╞════════════╪════════════╪═════════╪═══════╪═══╪════════════╪════════════╪════════════╪═══════════╡\n",
      "│ 2020-01-01 ┆ 0          ┆ 3564720 ┆ (2011 ┆ … ┆ -0.049103  ┆ 0.208663   ┆ 2.932496   ┆ 17.208398 │\n",
      "│            ┆            ┆         ┆ HS60) ┆   ┆            ┆            ┆            ┆           │\n",
      "│ 2020-01-01 ┆ 1          ┆ 3591759 ┆ (2011 ┆ … ┆ -0.986705  ┆ 0.038473   ┆ 2.623239   ┆ 16.040134 │\n",
      "│            ┆            ┆         ┆ YE40) ┆   ┆            ┆            ┆            ┆           │\n",
      "│ 2020-01-01 ┆ 2          ┆ 3630817 ┆ (2013 ┆ … ┆ -0.301277  ┆ 0.006793   ┆ 1.333342   ┆ 17.003418 │\n",
      "│            ┆            ┆         ┆ EC20) ┆   ┆            ┆            ┆            ┆           │\n",
      "└────────────┴────────────┴─────────┴───────┴───┴────────────┴────────────┴────────────┴───────────┘\n"
     ]
    }
   ],
   "source": [
    "print(\"=== DATA SHAPE & TYPES ===\")\n",
    "print(f\"Shape: {new_data.shape}\")\n",
    "print(f\"Columns: {len(new_data.columns)}\")\n",
    "print(\"\\nData types:\")\n",
    "print(new_data.dtypes)\n",
    "\n",
    "print(\"\\n=== TARGET VARIABLE ===\")\n",
    "print(f\"Target distribution:\")\n",
    "print(new_data['is_potentially_hazardous'].value_counts())\n",
    "\n",
    "print(\"\\n=== MISSING VALUES ===\")\n",
    "null_counts = new_data.null_count().to_series().to_list()\n",
    "print(\"Columns with missing values:\")\n",
    "print(null_counts)\n",
    "\n",
    "print(\"\\n=== SAMPLE ROWS ===\")\n",
    "print(new_data.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "750b867c",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols = new_data.select(pl.col(pl.Float64, pl.Int64, pl.Int32, pl.Int8, pl.Int16)).columns\n",
    "numerical_cols = [col for col in numerical_cols if col not in ['obj_that_day', 'epoch_date_close_approach']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74696e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection_corr(data):\n",
    "    \n",
    "\n",
    "    correlations = []\n",
    "    for col in numerical_cols:\n",
    "        try:\n",
    "            corr = data.select(pl.corr('is_potentially_hazardous', col)).item()\n",
    "            if corr is not None:\n",
    "                correlations.append((col, abs(col)))\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    correlations.sort(key=lambda x: x[1], reverse=True)\n",
    "    return correlations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7970bc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations = feature_selection_corr(new_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "54fcec78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "temp_data = new_data.select(numerical_cols)\n",
    "\n",
    "correlations = {}\n",
    "for col in numerical_cols:\n",
    "    corr = new_data.select(pl.corr('is_potentially_hazardous', col)).item()\n",
    "    correlations[col] = corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "658ec597",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = [\n",
    "    # Strongest predictors (>0.3 correlation)\n",
    "    'threat_score',           # 0.351 - Best overall predictor!\n",
    "    'impact_potential',       # 0.345 - Physics-based threat\n",
    "    'size_percentile',        # 0.403 - Relative size ranking\n",
    "    'log_diameter',           # 0.301 - Transformed size measure\n",
    "    \n",
    "    # Strong predictors (0.2-0.3 correlation)  \n",
    "    'avg_diameter_km',        # 0.244 - Core size measure\n",
    "    'velocity_zscore',        # 0.203 - Normalized velocity\n",
    "    'relative_velocity_km/sec', # 0.203 - Core velocity\n",
    "    'log_velocity',           # 0.226 - Transformed velocity\n",
    "    'velocity_percentile',    # 0.235 - Relative velocity ranking\n",
    "    \n",
    "    # Supporting features (0.15-0.2)\n",
    "    'size_velocity_product',  # 0.153 - Size × speed interaction\n",
    "    \n",
    "    # Key categorical/expert features\n",
    "    'is_sentry_object',       # Expert NASA assessment (boolean)\n",
    "    \n",
    "    # Negative correlation (important!)\n",
    "    'absolute_magniutude_h',  # -0.377 - Brightness (smaller=brighter=larger!)\n",
    "    'brightness_size_ratio'   # -0.212 - Physical relationship\n",
    "    ]\n",
    "def train_test_split(data):\n",
    "    X = data.select(selected_features)\n",
    "    y = data.select('is_potentially_hazardous')\n",
    "\n",
    "    X_pd = X.to_pandas()\n",
    "    y_pd = y.to_pandas().squeeze()\n",
    "\n",
    "    from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_pd, y_pd,\n",
    "        test_size=0.25,\n",
    "        stratify=y_pd,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "d9c2673d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, cv = train_test_split(data=new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "aa95ceb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "def smart_preprocessing(X_train, X_test):\n",
    "    boolean_features = ['is_sentry_object']\n",
    "    numerical_features = [col for col in selected_features if col not in boolean_features]\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), numerical_features),\n",
    "            ('bool', 'passthrough', boolean_features)\n",
    "        ],\n",
    "        remainder='drop'\n",
    "    )\n",
    "\n",
    "    X_train_processed = preprocessor.fit_transform(X_train)\n",
    "    X_test_processed = preprocessor.fit_transform(X_test)\n",
    "\n",
    "    return X_train_processed, X_test_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "54a26fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_processed, X_test_processed = smart_preprocessing(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "9f2e1f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, f1_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def baseline_model(X_train_processed, X_test_processed, y_train, y_test, cv):\n",
    "    # Model 1: Random Forest\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=200, # More trees for better performance\n",
    "        class_weight='balanced', # Handle 12 vs 114 imbalance\n",
    "        random_state=42,\n",
    "        max_depth=4, # Shallow to prevent overfitting\n",
    "        min_samples_split=5, # Conservastive splitting\n",
    "        min_samples_leaf=2 # Ensure meaning full leaves\n",
    "    )\n",
    "\n",
    "    # Cross val scores\n",
    "    rf_cv_scores = cross_val_score(rf, X_train_processed, y_train, cv=cv, scoring='f1')\n",
    "    print(f\"F1 Scores: {rf_cv_scores}\")\n",
    "    print(f\"Mean F1: {rf_cv_scores.mean():.3f} (+/- {rf_cv_scores.std() * 2:.3f})\")\n",
    "\n",
    "    # Fit and evaluate\n",
    "    rf.fit(X_train_processed, y_train)\n",
    "    rf_pred = rf.predict(X_test_processed)\n",
    "    rf_prob = rf.predict_proba(X_test_processed)[:, 1]\n",
    "    print(\"-------------RANDOM FOREST TEST RESULTS-------------\")\n",
    "    print(classification_report(y_test, rf_pred))\n",
    "    print(f\"ROC-AUC: {roc_auc_score(y_test, rf_prob):.3f}\")\n",
    "\n",
    "    # Model 2: Logistic Regression\n",
    "    lr = LogisticRegression(\n",
    "        class_weight='balanced',\n",
    "        random_state=42,\n",
    "        max_iter=2000,\n",
    "        C=0.1\n",
    "    )\n",
    "\n",
    "    lr_cv_scores = cross_val_score(lr, X_train_processed, y_train, cv=cv, scoring='f1')\n",
    "    print(f\"F1 Scores: {lr_cv_scores}\")\n",
    "    print(f\"Mean F1: {lr_cv_scores.mean():.3f} (+/- {lr_cv_scores.std() * 2:.3f})\")\n",
    "\n",
    "    lr.fit(X_train_processed, y_train)\n",
    "    lr_pred = lr.predict(X_test_processed)\n",
    "    lr_prob = lr.predict_proba(X_test_processed)[:, 1]\n",
    "    print(\"-------------LOGISITIC REGRESSION TEST RESULTS-------------\")\n",
    "    print(classification_report(y_test, lr_pred))\n",
    "    print(f\"ROC-AUC: {roc_auc_score(y_test, lr_prob):.3f}\")\n",
    "\n",
    "    return rf_pred, rf_prob, lr_pred, lr_prob, rf, lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "7f1e2040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Scores: [0.66666667 0.5        0.66666667 0.         0.5       ]\n",
      "Mean F1: 0.467 (+/- 0.490)\n",
      "-------------RANDOM FOREST TEST RESULTS-------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.93      0.97      0.95        29\n",
      "        True       0.50      0.33      0.40         3\n",
      "\n",
      "    accuracy                           0.91        32\n",
      "   macro avg       0.72      0.65      0.67        32\n",
      "weighted avg       0.89      0.91      0.90        32\n",
      "\n",
      "ROC-AUC: 0.908\n",
      "F1 Scores: [0.5        0.66666667 0.66666667 0.66666667 0.4       ]\n",
      "Mean F1: 0.580 (+/- 0.222)\n",
      "-------------LOGISITIC REGRESSION TEST RESULTS-------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.96      0.83      0.89        29\n",
      "        True       0.29      0.67      0.40         3\n",
      "\n",
      "    accuracy                           0.81        32\n",
      "   macro avg       0.62      0.75      0.64        32\n",
      "weighted avg       0.90      0.81      0.84        32\n",
      "\n",
      "ROC-AUC: 0.885\n"
     ]
    }
   ],
   "source": [
    "rf_pred, rf_prob, lr_pred, lr_prob, rf, lr = baseline_model(X_train_processed=X_train_processed, X_test_processed=X_test_processed, y_train=y_train, y_test=y_test, cv=cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f85912",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def feature_importance(model):\n",
    "    feature_names = [col for col in selected_features]\n",
    "    importances = model.feature_importances_\n",
    "\n",
    "    feature_importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': importances\n",
    "    }).sort_values('importance', ascending=False)\n",
    "\n",
    "    return feature_importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "cd2da5f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>absolute_magniutude_h</td>\n",
       "      <td>1.451242e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>is_sentry_object</td>\n",
       "      <td>1.335752e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>size_velocity_product</td>\n",
       "      <td>1.331765e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>threat_score</td>\n",
       "      <td>1.248301e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>size_percentile</td>\n",
       "      <td>1.022533e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>avg_diameter_km</td>\n",
       "      <td>8.831750e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>impact_potential</td>\n",
       "      <td>8.515958e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>log_diameter</td>\n",
       "      <td>7.042571e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>log_velocity</td>\n",
       "      <td>3.584730e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>relative_velocity_km/sec</td>\n",
       "      <td>2.963773e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>velocity_percentile</td>\n",
       "      <td>2.782567e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>velocity_zscore</td>\n",
       "      <td>2.382726e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>brightness_size_ratio</td>\n",
       "      <td>1.667953e-17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     feature    importance\n",
       "11     absolute_magniutude_h  1.451242e-01\n",
       "10          is_sentry_object  1.335752e-01\n",
       "9      size_velocity_product  1.331765e-01\n",
       "0               threat_score  1.248301e-01\n",
       "2            size_percentile  1.022533e-01\n",
       "4            avg_diameter_km  8.831750e-02\n",
       "1           impact_potential  8.515958e-02\n",
       "3               log_diameter  7.042571e-02\n",
       "7               log_velocity  3.584730e-02\n",
       "6   relative_velocity_km/sec  2.963773e-02\n",
       "8        velocity_percentile  2.782567e-02\n",
       "5            velocity_zscore  2.382726e-02\n",
       "12     brightness_size_ratio  1.667953e-17"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importance(rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "ab371137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-3.0.2-py3-none-win_amd64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: numpy in c:\\python313\\lib\\site-packages (from xgboost) (2.3.1)\n",
      "Requirement already satisfied: scipy in c:\\python313\\lib\\site-packages (from xgboost) (1.16.0)\n",
      "Downloading xgboost-3.0.2-py3-none-win_amd64.whl (150.0 MB)\n",
      "   ---------------------------------------- 0.0/150.0 MB ? eta -:--:--\n",
      "    --------------------------------------- 2.1/150.0 MB 11.0 MB/s eta 0:00:14\n",
      "   - -------------------------------------- 4.2/150.0 MB 10.8 MB/s eta 0:00:14\n",
      "   - -------------------------------------- 6.6/150.0 MB 10.8 MB/s eta 0:00:14\n",
      "   -- ------------------------------------- 8.9/150.0 MB 10.7 MB/s eta 0:00:14\n",
      "   -- ------------------------------------- 11.0/150.0 MB 10.8 MB/s eta 0:00:13\n",
      "   --- ------------------------------------ 13.1/150.0 MB 10.8 MB/s eta 0:00:13\n",
      "   ---- ----------------------------------- 15.5/150.0 MB 10.9 MB/s eta 0:00:13\n",
      "   ---- ----------------------------------- 17.8/150.0 MB 10.8 MB/s eta 0:00:13\n",
      "   ----- ---------------------------------- 19.7/150.0 MB 10.6 MB/s eta 0:00:13\n",
      "   ----- ---------------------------------- 21.8/150.0 MB 10.6 MB/s eta 0:00:13\n",
      "   ------ --------------------------------- 24.1/150.0 MB 10.6 MB/s eta 0:00:12\n",
      "   ------- -------------------------------- 26.5/150.0 MB 10.6 MB/s eta 0:00:12\n",
      "   ------- -------------------------------- 28.8/150.0 MB 10.7 MB/s eta 0:00:12\n",
      "   -------- ------------------------------- 30.9/150.0 MB 10.6 MB/s eta 0:00:12\n",
      "   -------- ------------------------------- 33.3/150.0 MB 10.7 MB/s eta 0:00:11\n",
      "   --------- ------------------------------ 35.7/150.0 MB 10.7 MB/s eta 0:00:11\n",
      "   ---------- ----------------------------- 37.7/150.0 MB 10.7 MB/s eta 0:00:11\n",
      "   ---------- ----------------------------- 39.8/150.0 MB 10.7 MB/s eta 0:00:11\n",
      "   ----------- ---------------------------- 42.2/150.0 MB 10.7 MB/s eta 0:00:11\n",
      "   ----------- ---------------------------- 44.6/150.0 MB 10.7 MB/s eta 0:00:10\n",
      "   ------------ --------------------------- 46.7/150.0 MB 10.7 MB/s eta 0:00:10\n",
      "   ------------- -------------------------- 49.0/150.0 MB 10.7 MB/s eta 0:00:10\n",
      "   ------------- -------------------------- 51.4/150.0 MB 10.8 MB/s eta 0:00:10\n",
      "   -------------- ------------------------- 53.7/150.0 MB 10.8 MB/s eta 0:00:09\n",
      "   -------------- ------------------------- 55.8/150.0 MB 10.8 MB/s eta 0:00:09\n",
      "   --------------- ------------------------ 57.9/150.0 MB 10.7 MB/s eta 0:00:09\n",
      "   ---------------- ----------------------- 60.0/150.0 MB 10.7 MB/s eta 0:00:09\n",
      "   ---------------- ----------------------- 62.4/150.0 MB 10.7 MB/s eta 0:00:09\n",
      "   ----------------- ---------------------- 65.0/150.0 MB 10.7 MB/s eta 0:00:08\n",
      "   ----------------- ---------------------- 67.4/150.0 MB 10.8 MB/s eta 0:00:08\n",
      "   ------------------ --------------------- 70.0/150.0 MB 10.8 MB/s eta 0:00:08\n",
      "   ------------------- -------------------- 72.4/150.0 MB 10.8 MB/s eta 0:00:08\n",
      "   ------------------- -------------------- 75.0/150.0 MB 10.9 MB/s eta 0:00:07\n",
      "   -------------------- ------------------- 77.3/150.0 MB 10.9 MB/s eta 0:00:07\n",
      "   --------------------- ------------------ 79.7/150.0 MB 10.9 MB/s eta 0:00:07\n",
      "   --------------------- ------------------ 82.3/150.0 MB 10.9 MB/s eta 0:00:07\n",
      "   ---------------------- ----------------- 84.7/150.0 MB 11.0 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 87.0/150.0 MB 11.0 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 89.1/150.0 MB 11.0 MB/s eta 0:00:06\n",
      "   ------------------------ --------------- 91.5/150.0 MB 11.0 MB/s eta 0:00:06\n",
      "   ------------------------ --------------- 93.3/150.0 MB 10.9 MB/s eta 0:00:06\n",
      "   ------------------------- -------------- 95.7/150.0 MB 10.9 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 98.0/150.0 MB 10.9 MB/s eta 0:00:05\n",
      "   -------------------------- ------------ 100.4/150.0 MB 10.9 MB/s eta 0:00:05\n",
      "   -------------------------- ------------ 102.8/150.0 MB 11.0 MB/s eta 0:00:05\n",
      "   --------------------------- ----------- 105.4/150.0 MB 11.0 MB/s eta 0:00:05\n",
      "   ---------------------------- ---------- 107.7/150.0 MB 11.0 MB/s eta 0:00:04\n",
      "   ---------------------------- ---------- 110.1/150.0 MB 11.0 MB/s eta 0:00:04\n",
      "   ----------------------------- --------- 112.5/150.0 MB 11.0 MB/s eta 0:00:04\n",
      "   ----------------------------- --------- 114.8/150.0 MB 11.0 MB/s eta 0:00:04\n",
      "   ------------------------------ -------- 117.4/150.0 MB 11.1 MB/s eta 0:00:03\n",
      "   ------------------------------- ------- 119.8/150.0 MB 11.1 MB/s eta 0:00:03\n",
      "   ------------------------------- ------- 122.2/150.0 MB 11.1 MB/s eta 0:00:03\n",
      "   -------------------------------- ------ 124.5/150.0 MB 11.1 MB/s eta 0:00:03\n",
      "   --------------------------------- ----- 127.1/150.0 MB 11.1 MB/s eta 0:00:03\n",
      "   --------------------------------- ----- 129.5/150.0 MB 11.1 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 131.9/150.0 MB 11.1 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 134.5/150.0 MB 11.1 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 136.8/150.0 MB 11.2 MB/s eta 0:00:02\n",
      "   ------------------------------------ -- 139.2/150.0 MB 11.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 141.6/150.0 MB 11.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 143.9/150.0 MB 11.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  146.5/150.0 MB 11.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  148.9/150.0 MB 11.2 MB/s eta 0:00:01\n",
      "   --------------------------------------- 150.0/150.0 MB 11.1 MB/s eta 0:00:00\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-3.0.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "0cdd877d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "import xgboost as xgb\n",
    "\n",
    "def xgb_model():\n",
    "    # XGBBoost\n",
    "    xgb_model = xgb.XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=3,\n",
    "        learning_rate=0.1,\n",
    "        scale_pos_weight=len(y_train[y_train == False]) / len(y_train[y_train == False]),\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    xgb_cv_scores = cross_val_score(xgb_model, X_train_processed, y_train, cv=cv, scoring='f1')\n",
    "    print(\"🚀 XGBOOST CROSS-VALIDATION:\")\n",
    "    print(f\"Mean F1: {xgb_cv_scores.mean():.3f} (+/- {xgb_cv_scores.std() * 2:.3f})\")\n",
    "\n",
    "    # Test performance\n",
    "    xgb_model.fit(X_train_processed, y_train)\n",
    "    xgb_pred = xgb_model.predict(X_test_processed)\n",
    "    xgb_prob = xgb_model.predict_proba(X_test_processed)[:, 1]\n",
    "\n",
    "    print(\"\\n🚀 XGBOOST TEST RESULTS:\")\n",
    "    print(classification_report(y_test, xgb_pred))\n",
    "    print(f\"ROC-AUC: {roc_auc_score(y_test, xgb_prob):.3f}\")\n",
    "\n",
    "    return xgb_pred, xgb_prob, xgb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "041cb43e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 XGBOOST CROSS-VALIDATION:\n",
      "Mean F1: 0.133 (+/- 0.533)\n",
      "\n",
      "🚀 XGBOOST TEST RESULTS:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.93      0.97      0.95        29\n",
      "        True       0.50      0.33      0.40         3\n",
      "\n",
      "    accuracy                           0.91        32\n",
      "   macro avg       0.72      0.65      0.67        32\n",
      "weighted avg       0.89      0.91      0.90        32\n",
      "\n",
      "ROC-AUC: 0.931\n"
     ]
    }
   ],
   "source": [
    "xgb_pred, xgb_prob, xgb_model = xgb_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "1db20f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svc_model():\n",
    "    svm = SVC(\n",
    "        probability=True, \n",
    "        class_weight='balanced',\n",
    "        kernel='rbf',\n",
    "        C=1.0,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    svm_cross_scores = cross_val_score(svm, X_train_processed, y_train, cv=cv, scoring='f1')\n",
    "    print(\"🚀 SVM CROSS-VALIDATION:\")\n",
    "    print(f\"Mean F1: {svm_cross_scores.mean():.3f} (+/- {svm_cross_scores.std() * 2:.3f})\")\n",
    "\n",
    "    # Test performance\n",
    "    svm.fit(X_train_processed, y_train)\n",
    "    svm_pred = svm.predict(X_test_processed)\n",
    "    svm_prob = svm.predict_proba(X_test_processed)[:, 1]\n",
    "\n",
    "    print(\"\\n🚀 SVM TEST RESULTS:\")\n",
    "    print(classification_report(y_test, svm_pred))\n",
    "    print(f\"ROC-AUC: {roc_auc_score(y_test, svm_prob):.3f}\")\n",
    "\n",
    "    return svm_pred, svm_prob, svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "7d71615e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 SVM CROSS-VALIDATION:\n",
      "Mean F1: 0.482 (+/- 0.199)\n",
      "\n",
      "🚀 SVM TEST RESULTS:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.96      0.90      0.93        29\n",
      "        True       0.40      0.67      0.50         3\n",
      "\n",
      "    accuracy                           0.88        32\n",
      "   macro avg       0.68      0.78      0.71        32\n",
      "weighted avg       0.91      0.88      0.89        32\n",
      "\n",
      "ROC-AUC: 0.908\n"
     ]
    }
   ],
   "source": [
    "svm_pred, svm_prob, svm = svc_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "bf611d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_boosting_model():\n",
    "    gb = GradientBoostingClassifier(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=3,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    gb_cross_scores = cross_val_score(gb, X_train_processed, y_train, cv=cv, scoring='f1')\n",
    "    print(\"🚀 Gradient Boosting CROSS-VALIDATION:\")\n",
    "    print(f\"Mean F1: {gb_cross_scores.mean():.3f} (+/- {gb_cross_scores.std() * 2:.3f})\")\n",
    "\n",
    "    # Test performance\n",
    "    gb.fit(X_train_processed, y_train)\n",
    "    gb_pred = gb.predict(X_test_processed)\n",
    "    gb_prob = gb.predict_proba(X_test_processed)[:, 1]\n",
    "\n",
    "    print(\"\\n🚀 Gradient Boosting TEST RESULTS:\")\n",
    "    print(classification_report(y_test, gb_pred))\n",
    "    print(f\"ROC-AUC: {roc_auc_score(y_test, gb_prob):.3f}\")\n",
    "\n",
    "    return gb_pred, gb_prob, gb\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "1e46d757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Gradient Boosting CROSS-VALIDATION:\n",
      "Mean F1: 0.447 (+/- 0.491)\n",
      "\n",
      "🚀 Gradient Boosting TEST RESULTS:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.94      1.00      0.97        29\n",
      "        True       1.00      0.33      0.50         3\n",
      "\n",
      "    accuracy                           0.94        32\n",
      "   macro avg       0.97      0.67      0.73        32\n",
      "weighted avg       0.94      0.94      0.92        32\n",
      "\n",
      "ROC-AUC: 0.908\n"
     ]
    }
   ],
   "source": [
    "gb_pred, gb_prob, gb = gradient_boosting_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "f8fe90c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "def basic_neural_net():\n",
    "    mlp = MLPClassifier(\n",
    "        hidden_layer_sizes=(20,10),\n",
    "        max_iter=1000,\n",
    "        random_state=42,\n",
    "        early_stopping=True,\n",
    "        validation_fraction=0.2\n",
    "    )\n",
    "\n",
    "    mlp_cross_scores = cross_val_score(mlp, X_train_processed, y_train, cv=cv, scoring='f1')\n",
    "    print(\"🚀 Basic Neural Net CROSS-VALIDATION:\")\n",
    "    print(f\"Mean F1: {mlp_cross_scores.mean():.3f} (+/- {mlp_cross_scores.std() * 2:.3f})\")\n",
    "\n",
    "    # Test performance\n",
    "    mlp.fit(X_train_processed, y_train)\n",
    "    mlp_pred = mlp.predict(X_test_processed)\n",
    "    mlp_prob = mlp.predict_proba(X_test_processed)[:, 1]\n",
    "\n",
    "    print(\"\\n🚀 Basic Neural Net TEST RESULTS:\")\n",
    "    print(classification_report(y_test, mlp_pred))\n",
    "    print(f\"ROC-AUC: {roc_auc_score(y_test, mlp_prob):.3f}\")\n",
    "\n",
    "    return mlp_pred, mlp_prob, mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "41528cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Basic Neural Net CROSS-VALIDATION:\n",
      "Mean F1: 0.363 (+/- 0.325)\n",
      "\n",
      "🚀 Basic Neural Net TEST RESULTS:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       1.00      0.59      0.74        29\n",
      "        True       0.20      1.00      0.33         3\n",
      "\n",
      "    accuracy                           0.62        32\n",
      "   macro avg       0.60      0.79      0.54        32\n",
      "weighted avg       0.93      0.62      0.70        32\n",
      "\n",
      "ROC-AUC: 0.954\n"
     ]
    }
   ],
   "source": [
    "mlp_pred, mlp_prob, mlp = basic_neural_net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "1e46d264",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "def tune_svv():\n",
    "    svm_param_grid = {\n",
    "        'C': [0.1, 1, 10, 100],\n",
    "        'gamma': ['scale', 'auto', 0.01, 0.1, 1],\n",
    "        'kernel': ['rbf', 'poly'],\n",
    "        'class_weight': ['blanced', {False: 1, True: 5}, {False: 1, True: 10}]\n",
    "    }\n",
    "\n",
    "    svm_grid = GridSearchCV(\n",
    "        svm,\n",
    "        svm_param_grid,\n",
    "        cv=cv,\n",
    "        scoring='f1',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    print(f\"Tuning SVM\")\n",
    "    svm_grid.fit(X_train_processed, y_train)\n",
    "\n",
    "    print(\"🎯 BEST SVM PARAMETERS:\")\n",
    "    print(svm_grid.best_params_)\n",
    "    print(f\"Best CV F1: {svm_grid.best_score_:.3f}\")\n",
    "\n",
    "    best_svm = svm_grid.best_estimator_\n",
    "    best_svm_pred = best_svm.predict(X_test_processed)\n",
    "    best_svm_prob = best_svm.predict_proba(X_test_processed)[:, 1]\n",
    "\n",
    "    print('OPTIMISED SVM RESULTS')\n",
    "    print(classification_report(y_test, best_svm_pred))\n",
    "    print(f\"ROC-AUC: {roc_auc_score(y_test, best_svm_prob):.3f}\")\n",
    "\n",
    "    return best_svm_pred, best_svm_prob, best_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "4886e90f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning SVM\n",
      "🎯 BEST SVM PARAMETERS:\n",
      "{'C': 10, 'class_weight': {False: 1, True: 5}, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "Best CV F1: 0.581\n",
      "OPTIMISED SVM RESULTS\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.96      0.90      0.93        29\n",
      "        True       0.40      0.67      0.50         3\n",
      "\n",
      "    accuracy                           0.88        32\n",
      "   macro avg       0.68      0.78      0.71        32\n",
      "weighted avg       0.91      0.88      0.89        32\n",
      "\n",
      "ROC-AUC: 0.897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python313\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:516: FitFailedWarning: \n",
      "200 fits failed out of a total of 600.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "200 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Python313\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 859, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python313\\Lib\\site-packages\\sklearn\\base.py\", line 1356, in wrapper\n",
      "    estimator._validate_params()\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"c:\\Python313\\Lib\\site-packages\\sklearn\\base.py\", line 469, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        self._parameter_constraints,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        self.get_params(deep=False),\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        caller_name=self.__class__.__name__,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"c:\\Python313\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 98, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "    ...<2 lines>...\n",
      "    )\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'class_weight' parameter of SVC must be a str among {'balanced'}, an instance of 'dict' or None. Got 'blanced' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Python313\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1135: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan 0.50761905 0.1\n",
      " 0.48761905 0.13333333 0.         0.         0.37428571 0.18\n",
      " 0.         0.08       0.54984127 0.29333333 0.54984127 0.29333333\n",
      " 0.50793651 0.17343358 0.54984127 0.40761905 0.23333333 0.08\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan 0.42761905 0.42\n",
      " 0.42761905 0.42       0.55555556 0.         0.32761905 0.37428571\n",
      " 0.2        0.34666667 0.40761905 0.42761905 0.40761905 0.42761905\n",
      " 0.54984127 0.17343358 0.42761905 0.42761905 0.2        0.34666667\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan 0.39428571 0.37428571\n",
      " 0.44190476 0.37428571 0.58095238 0.         0.39428571 0.39428571\n",
      " 0.33333333 0.34666667 0.39428571 0.47428571 0.44190476 0.37428571\n",
      " 0.53555556 0.10352941 0.39428571 0.52761905 0.33333333 0.34666667\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan 0.34666667 0.28\n",
      " 0.34666667 0.26       0.32761905 0.18       0.42666667 0.08\n",
      " 0.33333333 0.34666667 0.34666667 0.28       0.34666667 0.26\n",
      " 0.40761905 0.40761905 0.42666667 0.08       0.33333333 0.34666667]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "svm_pred_new, svm_prob_new, svm_new_model = tune_svv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "b4f19e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "def svm_tuning_advanced():\n",
    "    svm_params = {\n",
    "        'C': [0.01, 0.1, 1, 10, 50, 100, 200],\n",
    "        'gamma':  ['scale', 'auto', 0.001, 0.01, 0.1, 1, 10],\n",
    "        'kernel': ['rbf', 'poly', 'sigmoid'],\n",
    "        'class_weight': [\n",
    "            'balanced',\n",
    "            {False: 1, True: 3},\n",
    "            {False: 1, True: 5},\n",
    "            {False: 1, True: 8},\n",
    "            {False: 1, True: 10},\n",
    "            {False: 1, True: 15},\n",
    "            {False: 1, True: 20},\n",
    "        ],\n",
    "        'degree': [2,3,4]\n",
    "    }\n",
    "\n",
    "    svm_random_search = RandomizedSearchCV(\n",
    "        svm_new_model,\n",
    "        svm_params,\n",
    "        n_iter=200,\n",
    "        cv=cv,\n",
    "        scoring='f1',\n",
    "        n_jobs=1,\n",
    "        random_state=42,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    print(f\"Starting SVM random search\")\n",
    "    svm_random_search.fit(X_train_processed, y_train)\n",
    "    print(\"\\n🏆 BEST SVM PARAMETERS:\")\n",
    "    print(svm_random_search.best_params_)\n",
    "    print(f\"Best CV F1: {svm_random_search.best_score_:.3f}\")\n",
    "\n",
    "    # Test best SVM\n",
    "    best_svm_tuned = svm_random_search.best_estimator_\n",
    "    svm_tuned_pred = best_svm_tuned.predict(X_test_processed)\n",
    "    svm_tuned_prob = best_svm_tuned.predict_proba(X_test_processed)[:, 1]\n",
    "\n",
    "    print(\"\\n🎯 OPTIMIZED SVM TEST RESULTS:\")\n",
    "    print(classification_report(y_test, svm_tuned_pred))\n",
    "    print(f\"ROC-AUC: {roc_auc_score(y_test, svm_tuned_prob):.3f}\")\n",
    "\n",
    "    return svm_tuned_pred, svm_tuned_prob, best_svm_tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "3bd0dfa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting SVM random search\n",
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n",
      "\n",
      "🏆 BEST SVM PARAMETERS:\n",
      "{'kernel': 'rbf', 'gamma': 0.01, 'degree': 4, 'class_weight': {False: 1, True: 15}, 'C': 10}\n",
      "Best CV F1: 0.589\n",
      "\n",
      "🎯 OPTIMIZED SVM TEST RESULTS:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.96      0.90      0.93        29\n",
      "        True       0.40      0.67      0.50         3\n",
      "\n",
      "    accuracy                           0.88        32\n",
      "   macro avg       0.68      0.78      0.71        32\n",
      "weighted avg       0.91      0.88      0.89        32\n",
      "\n",
      "ROC-AUC: 0.920\n"
     ]
    }
   ],
   "source": [
    "svm_tuned_pred, svm_tuned_prob, best_svm_tuned = svm_tuning_advanced()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "fc2c8db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest_tuning():\n",
    "    rf_params = {\n",
    "        'n_estimators': [100, 200, 300, 500, 800],\n",
    "        'max_depth': [3, 4, 5,6,7,8, None],\n",
    "        'min_samples_split': [2, 5, 10, 15, 20],\n",
    "        'min_samples_leaf': [1, 2, 4, 6, 8],\n",
    "        'max_features': ['sqrt', 'log2', 0.3, 0.5, 0.7],\n",
    "        'class_weight':[\n",
    "            'balanced',\n",
    "            'balanced_subsample',\n",
    "            {False: 1, True: 3},\n",
    "            {False: 1, True: 5},\n",
    "            {False: 1, True: 8},\n",
    "            {False: 1, True: 10},\n",
    "            {False: 1, True: 16}\n",
    "        ],\n",
    "        'bootstrap': [True, False],\n",
    "        'criterion': ['gini', 'entropy']\n",
    "    }\n",
    "\n",
    "    rf_random_search = RandomizedSearchCV(\n",
    "        rf,\n",
    "        rf_params,\n",
    "        n_iter=300,\n",
    "        cv=cv,\n",
    "        scoring='f1',\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        verbose=1\n",
    "    )\n",
    "    print(\"Starting Random Forest random search (this may take 15-20 minutes)...\")\n",
    "    rf_random_search.fit(X_train_processed, y_train)\n",
    "\n",
    "    print(\"\\n🏆 BEST RANDOM FOREST PARAMETERS:\")\n",
    "    print(rf_random_search.best_params_)\n",
    "    print(f\"Best CV F1: {rf_random_search.best_score_:.3f}\")\n",
    "\n",
    "    # Test best Random Forest\n",
    "    best_rf_tuned = rf_random_search.best_estimator_\n",
    "    rf_tuned_pred = best_rf_tuned.predict(X_test_processed)\n",
    "    rf_tuned_prob = best_rf_tuned.predict_proba(X_test_processed)[:, 1]\n",
    "\n",
    "    print(\"\\n🎯 OPTIMIZED RANDOM FOREST TEST RESULTS:\")\n",
    "    print(classification_report(y_test, rf_tuned_pred))\n",
    "    print(f\"ROC-AUC: {roc_auc_score(y_test, rf_tuned_prob):.3f}\")\n",
    "\n",
    "    return rf_tuned_pred, rf_tuned_prob, best_rf_tuned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "be804beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Random Forest random search (this may take 15-20 minutes)...\n",
      "Fitting 5 folds for each of 300 candidates, totalling 1500 fits\n",
      "\n",
      "🏆 BEST RANDOM FOREST PARAMETERS:\n",
      "{'n_estimators': 100, 'min_samples_split': 20, 'min_samples_leaf': 2, 'max_features': 'log2', 'max_depth': 7, 'criterion': 'entropy', 'class_weight': 'balanced_subsample', 'bootstrap': True}\n",
      "Best CV F1: 0.640\n",
      "\n",
      "🎯 OPTIMIZED RANDOM FOREST TEST RESULTS:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.93      0.90      0.91        29\n",
      "        True       0.25      0.33      0.29         3\n",
      "\n",
      "    accuracy                           0.84        32\n",
      "   macro avg       0.59      0.61      0.60        32\n",
      "weighted avg       0.86      0.84      0.85        32\n",
      "\n",
      "ROC-AUC: 0.897\n"
     ]
    }
   ],
   "source": [
    "rf_tuned_pred, rf_tuned_prob, best_rf_tuned = random_forest_tuning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "a65362d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "def feature_importance_analysis(model):\n",
    "    perm_importance = permutation_importance(\n",
    "        model, X_test_processed, y_test,\n",
    "        n_repeats=20, random_state=42, scoring='f1'\n",
    "    )\n",
    "\n",
    "    feature_importance_df = pd.DataFrame({\n",
    "        'feature': selected_features,\n",
    "        'importance': perm_importance.importances_mean,\n",
    "        'std': perm_importance.importances_std\n",
    "    }).sort_values('importance', ascending=False)\n",
    "\n",
    "    return feature_importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "0b0d238e",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance_df = feature_importance_analysis(best_svm_tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "84d585f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>threat_score</td>\n",
       "      <td>0.079563</td>\n",
       "      <td>0.103559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>impact_potential</td>\n",
       "      <td>0.047421</td>\n",
       "      <td>0.088985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>log_velocity</td>\n",
       "      <td>0.039286</td>\n",
       "      <td>0.128940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>log_diameter</td>\n",
       "      <td>0.024603</td>\n",
       "      <td>0.049621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>size_percentile</td>\n",
       "      <td>0.022738</td>\n",
       "      <td>0.250494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>absolute_magniutude_h</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>0.089214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>velocity_percentile</td>\n",
       "      <td>0.009167</td>\n",
       "      <td>0.151441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>avg_diameter_km</td>\n",
       "      <td>0.005556</td>\n",
       "      <td>0.016667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>relative_velocity_km/sec</td>\n",
       "      <td>0.001984</td>\n",
       "      <td>0.023660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>velocity_zscore</td>\n",
       "      <td>0.001984</td>\n",
       "      <td>0.023660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>size_velocity_product</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>brightness_size_ratio</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>is_sentry_object</td>\n",
       "      <td>-0.020119</td>\n",
       "      <td>0.148011</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     feature  importance       std\n",
       "0               threat_score    0.079563  0.103559\n",
       "1           impact_potential    0.047421  0.088985\n",
       "7               log_velocity    0.039286  0.128940\n",
       "3               log_diameter    0.024603  0.049621\n",
       "2            size_percentile    0.022738  0.250494\n",
       "11     absolute_magniutude_h    0.014286  0.089214\n",
       "8        velocity_percentile    0.009167  0.151441\n",
       "4            avg_diameter_km    0.005556  0.016667\n",
       "6   relative_velocity_km/sec    0.001984  0.023660\n",
       "5            velocity_zscore    0.001984  0.023660\n",
       "9      size_velocity_product    0.000000  0.000000\n",
       "12     brightness_size_ratio    0.000000  0.000000\n",
       "10          is_sentry_object   -0.020119  0.148011"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importance_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
